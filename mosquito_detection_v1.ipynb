{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272bfe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locatization\n",
    "# 1. create train function\n",
    "# 2. create loss functions\n",
    "# 3. create models\n",
    "# classification\n",
    "# 1. create train function\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import pandas as pd\n",
    "import torch as th \n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "import src.localization as lc\n",
    "import src.data_loader as dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=450, height=450),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='coco'))\n",
    "\n",
    "\n",
    "transformed = transform(image=image, bboxes=bboxes)\n",
    "transformed_image = transformed['image']\n",
    "transformed_bboxes = transformed['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb8ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "np.random.seed(123)\n",
    "HEIGHT, WIDTH = 720, 1280\n",
    "\n",
    "def random_bbox():\n",
    "    x1 = np.random.randint(low=0, high=WIDTH)\n",
    "    y1 = np.random.randint(low=0, high=HEIGHT)\n",
    "    x2 = np.random.randint(low=x1 + 1, high=WIDTH + 1)\n",
    "    y2 = np.random.randint(low=y1 + 1, high=HEIGHT + 1)\n",
    "    bbox_albu = A.convert_bbox_to_albumentations([x1, y1, x2, y2], source_format='pascal_voc', rows=HEIGHT, cols=WIDTH)\n",
    "    bbox_yolo = A.convert_bbox_from_albumentations(bbox_albu, target_format='yolo', rows=HEIGHT, cols=WIDTH, check_validity=True)\n",
    "    # NOTE: at this point the bounding box has been checked to be valid.\n",
    "\n",
    "    return bbox_yolo\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [A.HorizontalFlip(), A.RandomBrightnessContrast()],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=[\"class_labels\"])\n",
    ")\n",
    "img = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(1000):\n",
    "    bboxes = [random_bbox()]\n",
    "    try:\n",
    "        transform(image=img, bboxes=bboxes, class_labels=[1])\n",
    "    except:\n",
    "        print(f\"[{i}] Invalid transformation of box: {str(bboxes[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d063c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BB 0: (65.0000, 100.0000, 200.0000, 150.0000) -> (126.5276, 169.5678, 215.3970, 202.4824)\n",
      "BB 1: (150.0000, 80.0000, 200.0000, 130.0000) -> (182.4824, 156.4020, 215.3970, 189.3166)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m image_before \u001b[38;5;241m=\u001b[39m bbs\u001b[38;5;241m.\u001b[39mdraw_on_image(image, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     38\u001b[0m image_after \u001b[38;5;241m=\u001b[39m bbs_aug\u001b[38;5;241m.\u001b[39mdraw_on_image(image_aug, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, color\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m])\n\u001b[0;32m---> 39\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "\n",
    "\n",
    "ia.seed(1)\n",
    "\n",
    "image = ia.quokka(size=(256, 256))\n",
    "bbs = BoundingBoxesOnImage([\n",
    "    BoundingBox(x1=65, y1=100, x2=200, y2=150),\n",
    "    BoundingBox(x1=150, y1=80, x2=200, y2=130)\n",
    "], shape=image.shape)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Multiply((1.2, 1.5)), # change brightness, doesn't affect BBs\n",
    "    iaa.Affine(\n",
    "        translate_px={\"x\": 40, \"y\": 60},\n",
    "        scale=(0.5, 0.7)\n",
    "    ) # translate by 40/60px on x/y axis, and scale to 50-70%, affects BBs\n",
    "])\n",
    "\n",
    "# Augment BBs and images.\n",
    "image_aug, bbs_aug = seq(image=image, bounding_boxes=bbs)\n",
    "\n",
    "# print coordinates before/after augmentation (see below)\n",
    "# use .x1_int, .y_int, ... to get integer coordinates\n",
    "for i in range(len(bbs.bounding_boxes)):\n",
    "    before = bbs.bounding_boxes[i]\n",
    "    after = bbs_aug.bounding_boxes[i]\n",
    "    print(\"BB %d: (%.4f, %.4f, %.4f, %.4f) -> (%.4f, %.4f, %.4f, %.4f)\" % (\n",
    "        i,\n",
    "        before.x1, before.y1, before.x2, before.y2,\n",
    "        after.x1, after.y1, after.x2, after.y2)\n",
    "    )\n",
    "\n",
    "# image with BBs before/after augmentation (shown below)\n",
    "image_before = bbs.draw_on_image(image, size=2)\n",
    "image_after = bbs_aug.draw_on_image(image_aug, size=2, color=[0, 0, 255])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '../data/train'\n",
    "annotations_csv = '../data/train.csv'\n",
    "annotations_df = pd.read_csv(annotations_csv)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(size=(224, 224), interpolation=T.InterpolationMode.BILINEAR),\n",
    "    T.ToTensor(), \n",
    "    T.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), \n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "class_dict = {\n",
    "    \"albopictus\": th.tensor([1, 0, 0, 0, 0, 0]),\n",
    "    \"culex\": th.tensor([0, 1, 0, 0, 0, 0]),\n",
    "    \"japonicus/koreicus\": th.tensor([0, 0, 1, 0, 0, 0]),\n",
    "    \"culiseta\": th.tensor([0, 0, 0, 1, 0, 0]),\n",
    "    \"anopheles\": th.tensor([0, 0, 0, 0, 1, 0]),\n",
    "    \"aegypti\": th.tensor([0, 0, 0, 0, 0, 1]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec225676",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lc.LocalizationNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b502cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lc.MosquitoLocalization()\n",
    "parameters_backbone = [\n",
    "    {'params': p, \"lr\": model.opt_params.get(\"lr\", 1e-4) * 0.01, \"weight_decay\": model.opt_params.get(\"weight_decay\", 1e-6) * 10}\n",
    "    for _, p in model.detector.backbone.named_parameters()\n",
    "]\n",
    "parameters_mlp = [\n",
    "    {'params': p, \"lr\": model.opt_params.get(\"lr\", 1e-4), \"weight_decay\": model.opt_params.get(\"weight_decay\", 1e-6)}\n",
    "    for _, p in model.detector.mlp.named_parameters()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52feee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = annotations_df.sample(frac=0.8,random_state=200)\n",
    "val_df = annotations_df.drop(train_df.index)\n",
    "\n",
    "train_dataset = dl.SimpleDetectionDataset(train_df, img_dir, class_dict, transform)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=64, \n",
    "                              shuffle=True, \n",
    "                              num_workers=12, \n",
    "                              persistent_workers=True, \n",
    "                              pin_memory=True)\n",
    "\n",
    "val_dataset = dl.SimpleDetectionDataset(val_df, img_dir, class_dict, transform)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                         batch_size=64, \n",
    "                          shuffle=False, \n",
    "                          num_workers=12, \n",
    "                          persistent_workers=True, \n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.fill_up_cache(annotations_df, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daaa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dl.CACHE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(train_dataloader):\n",
    "    pass\n",
    "\n",
    "print(len(dl.CACHE))\n",
    "\n",
    "for batch in tqdm(train_dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dl.CACHE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d869097",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_float32_matmul_precision('high')\n",
    "model = lc.MosquitoLocalization()\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", \n",
    "                     precision='16-mixed',\n",
    "                     max_epochs=10, \n",
    "                     logger=True)\n",
    "trainer.fit(model=model, \n",
    "            train_dataloaders=train_dataloader, \n",
    "            val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"bbox_norm\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b673c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"bbox_norm\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b65b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('facebook/dino-vitb16')\n",
    "model = ViTModel.from_pretrained('facebook/dino-vitb16')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')\n",
    "\n",
    "inputs = preprocess(image).reshape(1, 3, 224, 224)\n",
    "\n",
    "output = model.encode_image(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a432d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_localization_net(net_params = {}, opt_params = {}):\n",
    "    \n",
    "    model = th.compile(LocalizationNet(**net_params)).cuda()\n",
    "    optimizer = th.optim.Adam(model.parameters(), **opt_params)\n",
    "    \n",
    "    print('start training')\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = batch[\"img\"].cuda(), batch[\"bbox_norm\"].cuda()\n",
    "\n",
    "        y_p = model(x)\n",
    "        loss = mse_loss(y, y_p)\n",
    "        with th.no_grad():\n",
    "            iou = iou_loss(y, y_p)\n",
    "            \n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    return model\n",
    "train_localization_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ab80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
