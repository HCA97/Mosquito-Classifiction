{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24327d30-0191-4a92-b04f-7b7bf934db32",
   "metadata": {},
   "source": [
    "# YOLO and OWL-ViT Check Annotation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48b958e-134a-42e9-acb6-6d69c8d219bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "import PIL\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 108000001\n",
    "\n",
    "%matplotlib inline  \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2c050a-f9c2-48ec-b37b-d434ec2923b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"../data_round_2/final\"\n",
    "annotations_csv = \"../data_round_2/phase2_train_v0.csv\"\n",
    "yolo_path = 'yolo/runs/detect/classic/weights/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01d37b2-9798-46d4-9731-b74313be5fb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_img_label \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_fName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m      3\u001b[0m _train_data, _val_data \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      4\u001b[0m     df_img_label,\n\u001b[1;32m      5\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      6\u001b[0m     stratify\u001b[38;5;241m=\u001b[39mdf_img_label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_label\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m val_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(_val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_fName\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_img_label = df[[\"img_fName\", \"class_label\"]].drop_duplicates()\n",
    "\n",
    "_train_data, _val_data = train_test_split(\n",
    "    df_img_label,\n",
    "    test_size=0.2,\n",
    "    stratify=df_img_label[\"class_label\"],\n",
    "    random_state=200,\n",
    ")\n",
    "val_list = list(set(_val_data[\"img_fName\"]))\n",
    "df = df[df['img_fName'].isin(val_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aaecc11-8604-489a-8fb6-a954e8e1f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "det = YOLO(yolo_path, task=\"detect\")\n",
    "df = pd.read_csv(annotations_csv)\n",
    "\n",
    "\n",
    "#_, df = train_test_split(\n",
    "#        df,\n",
    "#        test_size=0.2,\n",
    "#        #stratify=df[\"class_label\"],\n",
    "#        random_state=200,\n",
    "#    )\n",
    "\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=200)\n",
    "df = df.drop(train_df.index)\n",
    "\n",
    "owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/')\n",
    "owl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfaacbd-5502-4a4d-96cb-6d138a9355d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b69489a-ae9a-4254-a8fb-0cf7f712ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_pytorch(P : torch.tensor ,thresh_iou : float):\n",
    "    \"\"\"\n",
    "    Apply non-maximum suppression to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object.\n",
    "    Args:\n",
    "        boxes: (tensor) The location preds for the image \n",
    "            along with the class predscores, Shape: [num_boxes,5].\n",
    "        thresh_iou: (float) The overlap thresh for suppressing unnecessary boxes.\n",
    "    Returns:\n",
    "        A list of filtered boxes, Shape: [ , 5]\n",
    "\n",
    "    Not smart so copy paste:\n",
    "        https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/\n",
    "    \"\"\"\n",
    " \n",
    "    # we extract coordinates for every \n",
    "    # prediction box present in P\n",
    "    x1 = P[:, 0]\n",
    "    y1 = P[:, 1]\n",
    "    x2 = P[:, 2]\n",
    "    y2 = P[:, 3]\n",
    " \n",
    "    # we extract the confidence scores as well\n",
    "    scores = P[:, 4]\n",
    " \n",
    "    # calculate area of every block in P\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "     \n",
    "    # sort the prediction boxes in P\n",
    "    # according to their confidence scores\n",
    "    order = scores.argsort()\n",
    " \n",
    "    # initialise an empty list for \n",
    "    # filtered prediction boxes\n",
    "    keep = []\n",
    "     \n",
    " \n",
    "    while len(order) > 0:\n",
    "         \n",
    "        # extract the index of the \n",
    "        # prediction with highest score\n",
    "        # we call this prediction S\n",
    "        idx = order[-1]\n",
    " \n",
    "        # push S in filtered predictions list\n",
    "        keep.append(P[idx])\n",
    " \n",
    "        # remove S from P\n",
    "        order = order[:-1]\n",
    " \n",
    "        # sanity check\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "         \n",
    "        # select coordinates of BBoxes according to \n",
    "        # the indices in order\n",
    "        xx1 = torch.index_select(x1,dim = 0, index = order)\n",
    "        xx2 = torch.index_select(x2,dim = 0, index = order)\n",
    "        yy1 = torch.index_select(y1,dim = 0, index = order)\n",
    "        yy2 = torch.index_select(y2,dim = 0, index = order)\n",
    " \n",
    "        # find the coordinates of the intersection boxes\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    " \n",
    "        # find height and width of the intersection boxes\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "         \n",
    "        # take max with 0.0 to avoid negative w and h\n",
    "        # due to non-overlapping boxes\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    " \n",
    "        # find the intersection area\n",
    "        inter = w*h\n",
    " \n",
    "        # find the areas of BBoxes according the indices in order\n",
    "        rem_areas = torch.index_select(areas, dim = 0, index = order) \n",
    " \n",
    "        # find the union of every prediction T in P\n",
    "        # with the prediction S\n",
    "        # Note that areas[idx] represents area of S\n",
    "        union = (rem_areas - inter) + areas[idx]\n",
    "         \n",
    "        # find the IoU of every prediction in P with S\n",
    "        IoU = inter / union\n",
    " \n",
    "        # keep the boxes with IoU less than thresh_iou\n",
    "        mask = IoU < thresh_iou\n",
    "        order = order[mask]\n",
    "\n",
    "\n",
    "    boxes = [k.numpy().tolist()[:4] for k in keep]\n",
    "    scores = [k.numpy().tolist()[-1] for k in keep]\n",
    "    return boxes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77cffeb0-87b8-4028-a757-78f1383d79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def owl_detect_images(img_path, t=0.5):\n",
    "    image = Image.open(os.path.join(img_dir, img_path)).convert(\"RGB\")\n",
    "    texts = [[\"a photo of a mosquito\"]]\n",
    "    inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = owl_model(**inputs)\n",
    "    target_sizes = torch.Tensor([image.size[::-1]]).to('cuda')\n",
    "    results = owl_processor.post_process_object_detection(outputs=outputs, \n",
    "                                                          target_sizes=target_sizes, \n",
    "                                                          threshold=0.1)\n",
    "    boxes, scores = results[0][\"boxes\"].cpu().detach(), results[0][\"scores\"].cpu().detach()\n",
    "    #print('Before nms: ', len(boxes))\n",
    "\n",
    "    P = torch.cat((boxes, torch.unsqueeze(scores, 1)), 1)\n",
    "    boxes, scores = nms_pytorch(P, t)\n",
    "    #print('After nms: ', len(boxes))\n",
    "\n",
    "    return boxes, scores\n",
    "\n",
    "    \n",
    "def detect_images(img_path, t_iou=0.5, t_conf=0.5, shrink=5):\n",
    "    results = det(os.path.join(img_dir, img_path), iou=t_iou, verbose=False) \n",
    "\n",
    "    bboxes = []\n",
    "    confs = []\n",
    "\n",
    "    conf_max = 0.0\n",
    "    box_max = []\n",
    "    \n",
    "    for result in results:\n",
    "        _bboxes = result.boxes.xyxy.tolist()\n",
    "        _confs = result.boxes.conf.tolist()\n",
    "\n",
    "        for bbox, conf in zip(_bboxes, _confs):\n",
    "            #if conf > conf_max:\n",
    "            if conf > t_conf:\n",
    "                #conf_max = conf\n",
    "                #box_max = [bbox[0]+shrink, bbox[1]+shrink, bbox[2]-shrink, bbox[3]-shrink]\n",
    "                confs.append(conf)\n",
    "                bboxes.append(bbox)\n",
    "    #bboxes.append(box_max)\n",
    "    #confs.append(conf_max)\n",
    "                \n",
    "    return bboxes, confs\n",
    "\n",
    "\n",
    "def iou(box1, box2):\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    dif_x = (min(box1[2], box2[2]) - max(box1[0], box2[0]))\n",
    "    dif_y = (min(box1[3], box2[3]) - max(box1[1], box2[1]))\n",
    "    \n",
    "    inter = dif_x * dif_y\n",
    "    \n",
    "    if dif_x < 0 or dif_y < 0:\n",
    "        return 0\n",
    "    return inter / (area1 + area2 - inter)\n",
    "\n",
    "def plot_image(img_path, bboxes, true_box=[], plot=True, save_dir='examples'):\n",
    "    img = Image.open(os.path.join(img_dir, img_path)).convert(\"RGB\")\n",
    "    img_ = ImageDraw.Draw(img)  \n",
    "    \n",
    "    for bbox in bboxes:    \n",
    "        img_.rectangle(bbox, fill=None, outline=\"blue\", width=int(0.005*max(img.size)))\n",
    "\n",
    "    if true_box:\n",
    "        img_.rectangle(true_box, fill=None, outline=\"green\", width=int(0.005*max(img.size)))\n",
    "\n",
    "    \n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(img)\n",
    "    else:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        img.save(os.path.join(save_dir, img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "936e9ac1-cabe-49cb-b484-d1157cfec938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 2071/2071 [01:34<00:00, 21.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# find all the annotations \n",
    "\n",
    "__model = \"yolo\" # \"owl\"\n",
    "annotations_owl_path = f'annoations_{__model}.json'\n",
    "\n",
    "if not os.path.exists(annotations_owl_path):\n",
    "    data = {}\n",
    "    \n",
    "    for img_path in tqdm(df.img_fName):\n",
    "        if __model == \"owl\":\n",
    "            boxes, scores = owl_detect_images(img_path, t=0.1) # this should be equivalent\n",
    "        else:\n",
    "            boxes, scores = detect_images(img_path, t_conf=0.4, shrink=0)\n",
    "        data[img_path] = boxes\n",
    "\n",
    "    with open(annotations_owl_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "else:\n",
    "    data = json.load(open(annotations_owl_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7b9124-babb-46e0-8501-cf4e9ebaa992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do again so I don't run 20 min of owl guy\n",
    "\n",
    "for key, boxes in data.items():\n",
    "    if len(boxes) > 1:\n",
    "        P = torch.tensor([box + [len(boxes) - i] for i, box in enumerate(boxes)])\n",
    "        boxes, _ = nms_pytorch(P, 0.1)\n",
    "        data[key] = boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faf61aad-db9c-4c04-a2df-a568e5e3518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2071/2071 [00:00<00:00, 4826.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# find bad annotations\n",
    "\n",
    "def find_bad_annotations(df, data, t=0.1, shrink=0):\n",
    "    bad_annotations = []\n",
    "\n",
    "\n",
    "    for img_name, pred_boxes in tqdm(data.items()):\n",
    "        \n",
    "        # god i am bad at pandas\n",
    "        true_box = df[df.img_fName == img_name].values.tolist()[0][3:7]\n",
    "\n",
    "        good_annotation = False\n",
    "        for pred_box in pred_boxes:\n",
    "            if not pred_box:\n",
    "                continue\n",
    "\n",
    "            pred_box = [pred_box[0]+shrink, pred_box[1]+shrink, pred_box[2]-shrink, pred_box[3]-shrink]\n",
    "            if iou(true_box, pred_box) > t:\n",
    "                good_annotation = True\n",
    "                break\n",
    "\n",
    "        if not good_annotation:\n",
    "            bad_annotations.append(img_name)\n",
    "\n",
    "    return bad_annotations\n",
    "\n",
    "\n",
    "bad_annotations = find_bad_annotations(df, data, 0.75, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b40e38ef-2b12-4bfc-affb-8bd53059bf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236, 2071)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_annotations), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72e71589-92eb-4024-ae25-6832340e64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 236/236 [00:07<00:00, 31.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_name in tqdm(bad_annotations):\n",
    "    boxes = data[img_name]\n",
    "    if boxes == [[]]:\n",
    "        boxes = []\n",
    "    true_box = df[df.img_fName == img_name].values.tolist()[0][3:7]\n",
    "    plot_image(img_name, boxes, true_box, False, f'examples_failed_cases_{__model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1caaf89-64e6-496d-abd4-e0b913e9396e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show images with multiple detection\n",
    "data_multiple = {}\n",
    "for k, v in data.items():\n",
    "    if len(v) > 1:\n",
    "        data_multiple[k] = v\n",
    "len(data_multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2d6f92-4af1-4030-b060-7b7cb9281635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_name, boxes in tqdm(data_multiple.items()):\n",
    "    true_box = df[df.img_fName == img_name].values.tolist()[0][3:7]\n",
    "    plot_image(img_name, boxes, true_box, False, f'example_multiple_annotations_{__model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fdb10-d574-4833-a684-be6eec78cf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
