{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/working/lightning_logs/version_14/checkpoints/","metadata":{"execution":{"iopub.status.busy":"2023-10-20T14:08:22.980056Z","iopub.execute_input":"2023-10-20T14:08:22.980432Z","iopub.status.idle":"2023-10-20T14:08:23.932969Z","shell.execute_reply.started":"2023-10-20T14:08:22.980402Z","shell.execute_reply":"2023-10-20T14:08:23.931702Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"'epoch=0-val_loss=1.3399014472961426-val_f1_score=0.8580293655395508-val_multiclass_accuracy=0.8747615814208984.ckpt'\n","output_type":"stream"}]},{"cell_type":"code","source":"# !rm -rf /kaggle/working/lightning_logs/version_1","metadata":{"execution":{"iopub.status.busy":"2023-10-20T12:51:50.246003Z","iopub.execute_input":"2023-10-20T12:51:50.246980Z","iopub.status.idle":"2023-10-20T12:51:51.576401Z","shell.execute_reply.started":"2023-10-20T12:51:50.246942Z","shell.execute_reply":"2023-10-20T12:51:51.575355Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'lightning_logs/version_6/checkpoints/epoch=7-val_loss=0.7511404752731323-val_f1_score=0.9214637875556946-val_multiclass_accuracy=0.9239795207977295.ckpt')\n# FileLink(r'models/version_4_averaged.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lightning==2.0.8 --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-10-20T14:18:04.118062Z","iopub.execute_input":"2023-10-20T14:18:04.118466Z","iopub.status.idle":"2023-10-20T14:18:16.420189Z","shell.execute_reply.started":"2023-10-20T14:18:04.118437Z","shell.execute_reply":"2023-10-20T14:18:16.419039Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting lightning==2.0.8\n  Downloading lightning-2.0.8-py3-none-any.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Jinja2<5.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (3.1.2)\nRequirement already satisfied: PyYAML<8.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (6.0)\nRequirement already satisfied: arrow<3.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (1.2.3)\nRequirement already satisfied: backoff<4.0,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (2.2.1)\nRequirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (4.12.2)\nRequirement already satisfied: click<10.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (8.1.3)\nCollecting croniter<1.5.0,>=1.3.0 (from lightning==2.0.8)\n  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\nCollecting dateutils<2.0 (from lightning==2.0.8)\n  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\nCollecting deepdiff<8.0,>=5.7.0 (from lightning==2.0.8)\n  Downloading deepdiff-6.6.1-py3-none-any.whl (73 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fastapi<2.0,>=0.92.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (0.98.0)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (2023.6.0)\nCollecting inquirer<5.0,>=2.10.0 (from lightning==2.0.8)\n  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\nCollecting lightning-cloud>=0.5.37 (from lightning==2.0.8)\n  Downloading lightning_cloud-0.5.42-py3-none-any.whl (738 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.8/738.8 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (0.9.0)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (1.23.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (21.3)\nRequirement already satisfied: psutil<7.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (5.9.3)\nRequirement already satisfied: pydantic<2.2.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (1.10.10)\nCollecting python-multipart<2.0,>=0.0.5 (from lightning==2.0.8)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<4.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (2.31.0)\nRequirement already satisfied: rich<15.0,>=12.3.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (13.4.2)\nRequirement already satisfied: starlette in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (0.27.0)\nCollecting starsessions<2.0,>=1.2.1 (from lightning==2.0.8)\n  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: torch<4.0,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (2.0.0)\nRequirement already satisfied: torchmetrics<2.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (1.0.0)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (4.65.0)\nRequirement already satisfied: traitlets<7.0,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (5.9.0)\nRequirement already satisfied: typing-extensions<6.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (4.6.3)\nRequirement already satisfied: urllib3<4.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (1.26.15)\nRequirement already satisfied: uvicorn<2.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (0.22.0)\nRequirement already satisfied: websocket-client<3.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (1.6.0)\nRequirement already satisfied: websockets<13.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (11.0.3)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning==2.0.8) (2.0.4)\nRequirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from arrow<3.0,>=1.2.0->lightning==2.0.8) (2.8.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning==2.0.8) (2.3.2.post1)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from dateutils<2.0->lightning==2.0.8) (2023.3)\nCollecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning==2.0.8)\n  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec<2025.0,>=2022.5.0->lightning==2.0.8) (3.8.4)\nRequirement already satisfied: blessed>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.8) (1.20.0)\nCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning==2.0.8)\n  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\nCollecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning==2.0.8)\n  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<5.0->lightning==2.0.8) (2.1.3)\nRequirement already satisfied: pyjwt in /opt/conda/lib/python3.10/site-packages (from lightning-cloud>=0.5.37->lightning==2.0.8) (2.7.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from lightning-cloud>=0.5.37->lightning==2.0.8) (1.16.0)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from lightning-cloud>=0.5.37->lightning==2.0.8) (1.26.100)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->lightning==2.0.8) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<4.0->lightning==2.0.8) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<4.0->lightning==2.0.8) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<4.0->lightning==2.0.8) (2023.5.7)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<15.0,>=12.3.0->lightning==2.0.8) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<15.0,>=12.3.0->lightning==2.0.8) (2.15.1)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette->lightning==2.0.8) (3.7.0)\nRequirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from starsessions<2.0,>=1.2.1->lightning==2.0.8) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning==2.0.8) (3.1)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn<2.0->lightning==2.0.8) (0.14.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.8) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.8) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.8) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.8) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.8) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.8) (1.3.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->lightning==2.0.8) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->lightning==2.0.8) (1.1.1)\nRequirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning==2.0.8) (0.2.6)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning==2.0.8) (0.1.0)\nRequirement already satisfied: setuptools>=41.0 in /opt/conda/lib/python3.10/site-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning==2.0.8) (59.8.0)\nRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3->lightning-cloud>=0.5.37->lightning==2.0.8) (1.29.161)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->lightning-cloud>=0.5.37->lightning==2.0.8) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->lightning-cloud>=0.5.37->lightning==2.0.8) (0.6.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.11.0->lightning==2.0.8) (1.3.0)\nInstalling collected packages: python-editor, readchar, python-multipart, ordered-set, inquirer, deepdiff, dateutils, croniter, starsessions, lightning-cloud, lightning\nSuccessfully installed croniter-1.4.1 dateutils-0.6.12 deepdiff-6.6.1 inquirer-3.1.3 lightning-2.0.8 lightning-cloud-0.5.42 ordered-set-4.1.0 python-editor-1.0.4 python-multipart-0.0.6 readchar-4.0.5 starsessions-1.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torcheval","metadata":{"execution":{"iopub.status.busy":"2023-10-20T14:18:16.422072Z","iopub.execute_input":"2023-10-20T14:18:16.422353Z","iopub.status.idle":"2023-10-20T14:18:24.898725Z","shell.execute_reply.started":"2023-10-20T14:18:16.422330Z","shell.execute_reply":"2023-10-20T14:18:24.897812Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting torcheval\n  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.6.3)\nInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"execution":{"iopub.status.busy":"2023-10-20T14:18:24.900109Z","iopub.execute_input":"2023-10-20T14:18:24.900471Z","iopub.status.idle":"2023-10-20T14:18:33.646747Z","shell.execute_reply.started":"2023-10-20T14:18:24.900439Z","shell.execute_reply":"2023-10-20T14:18:33.645693Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting open_clip_torch\n  Downloading open_clip_torch-2.22.0-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.15.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2023.6.3)\nCollecting ftfy (from open_clip_torch)\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.65.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.16.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.1.99)\nRequirement already satisfied: protobuf<4 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (3.20.3)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.9.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.6)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->open_clip_torch) (0.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.23.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\nInstalling collected packages: ftfy, open_clip_torch\nSuccessfully installed ftfy-6.1.1 open_clip_torch-2.22.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch_ema","metadata":{"execution":{"iopub.status.busy":"2023-10-20T14:18:33.648794Z","iopub.execute_input":"2023-10-20T14:18:33.649076Z","iopub.status.idle":"2023-10-20T14:18:42.362395Z","shell.execute_reply.started":"2023-10-20T14:18:33.649051Z","shell.execute_reply":"2023-10-20T14:18:42.361289Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting torch_ema\n  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torch_ema) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torch_ema) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torch_ema) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torch_ema) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torch_ema) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torch_ema) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torch_ema) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torch_ema) (1.3.0)\nInstalling collected packages: torch_ema\nSuccessfully installed torch_ema-0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import List, Dict, Callable, Optional, Union, Tuple, Any\n\nimport os\nimport logging\n\nimport random\n\nfrom copy import deepcopy\nfrom torch_ema import ExponentialMovingAverage\n\nimport torch as th\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, Callback\nfrom pytorch_lightning.utilities.types import STEP_OUTPUT\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy\nfrom transformers import get_linear_schedule_with_warmup\n\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as T\nimport albumentations as A\n\nimport open_clip","metadata":{"execution":{"iopub.status.busy":"2023-10-20T14:18:42.363733Z","iopub.execute_input":"2023-10-20T14:18:42.364003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = \"ViT-L-14\"\nPRETRAIN_DATASET = \"datacomp_xl_s13b_b90k\"\nBATCH_SIZE = 128\nHEAD_NUMBER = 4\nAUGMENTATION = \"hca\"\nFREEZE_BACKBONE = False\nWARMUP_STEPS = 0\nEPOCHS = 1\nLABEL_SMOOTHING = 0.1\nDROPOUT_RATE = 0.5\nUSE_LINEAR_SCHEDULE = True\nUSE_LAYER_NORM = True\nTEST_SIZE = 0.2\nUSE_ZENODO_DATA = False\nN_ZENODO_DATA = False\nNUM_TRAINING_STEPS = 12000\nUSE_CLASS_WEIGHTS = False\nUSE_INATURALIST_DATA = False\nUSE_LUX = False\nUSE_PRETRAINED = True\nHEAD_LR = 1e-3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class HeadV1(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(),\n            nn.LeakyReLU(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV4(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV8(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV7(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.LayerNorm(f_in),\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV5(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(p=0.9),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV6(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(p=0.75),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV2(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV3(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Linear(f_in, f_in, bias=False),\n            nn.BatchNorm1d(f_in),\n            nn.LeakyReLU(),\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_in, bias=False),\n            nn.BatchNorm1d(f_in),\n            nn.LeakyReLU(),\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass CLIPClassifier(nn.Module):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        data: str = \"datacomp_xl_s13b_b90k\",\n        head_version: int = 1,\n    ):\n        super().__init__()\n        self.backbone = open_clip.create_model(\n            model_name, pretrained=None, device=\"cpu\"\n        ).visual\n\n        if model_name == \"ViT-L-14\":\n            self.n = 768\n            self.lrs = dict(\n                back_lrs={\"8\": 1.25e-6, \"16\": 2.5e-6, \"20\": 5e-6, \"24\": 10e-6},\n                back_wd=1e-3,\n                hd_lr=HEAD_LR,\n                hd_wd=1e-6,\n            )\n        elif model_name == \"ViT-H-14\":\n            self.n = 1024\n        elif model_name == \"ViT-B-16\":\n            self.n = 512\n        else:\n            raise ValueError\n\n        if head_version == 2:\n            self.label = HeadV2(n_classes, self.n)\n        elif head_version == 3:\n            self.label = HeadV3(n_classes, self.n)\n        elif head_version == 4:\n            self.label = HeadV4(n_classes, self.n)\n        elif head_version == 5:\n            self.label = HeadV5(n_classes, self.n)\n        elif head_version == 6:\n            self.label = HeadV6(n_classes, self.n)\n        elif head_version == 7:\n            self.label = HeadV7(n_classes, self.n)\n        elif head_version == 8:\n            self.label = HeadV8(n_classes, self.n)\n        else:\n            self.label = HeadV1(n_classes, self.n)\n\n        self.n_classes = n_classes\n\n    def forward(self, x: th.tensor) -> th.tensor:\n        x = self.backbone(x)\n        return self.label(x)\n\n    def get_parameter_section(self, parameters, lr=None, wd=None):\n        # https://github.com/IvanAer/G-Universal-CLIP\n        parameter_settings = []\n\n        lr_is_dict = isinstance(lr, dict)\n        wd_is_dict = isinstance(wd, dict)\n\n        layer_no = None\n        for n, p in parameters:\n            for split in n.split(\".\"):\n                if split.isnumeric():\n                    layer_no = int(split)\n\n            if not layer_no:\n                layer_no = 0\n\n            if lr_is_dict:\n                for k, v in lr.items():\n                    if layer_no < int(k):\n                        temp_lr = v\n                        break\n            else:\n                temp_lr = lr\n\n            if wd_is_dict:\n                for k, v in wd.items():\n                    if layer_no < int(k):\n                        temp_wd = v\n                        break\n            else:\n                temp_wd = wd\n\n            parameter_setting = {\"params\": p, \"lr\": temp_lr, \"weight_decay\": temp_wd}\n            parameter_settings.append(parameter_setting)\n        return parameter_settings\n\n    def get_learnable_params(self) -> list:\n        back_lrs = self.lrs[\"back_lrs\"]\n        back_wd = self.lrs[\"back_wd\"]\n        hd_lr = self.lrs[\"hd_lr\"]\n        hd_wd = self.lrs[\"hd_wd\"]\n\n        parameter_settings = []\n\n        if back_lrs and back_wd:\n            parameter_settings.extend(\n                self.get_parameter_section(\n                    [(n, p) for n, p in self.backbone.named_parameters()],\n                    lr=back_lrs,\n                    wd=back_wd,\n                )\n            )\n\n        parameter_settings.extend(\n            self.get_parameter_section(\n                [(n, p) for n, p in self.label.named_parameters()], lr=hd_lr, wd=hd_wd\n            )\n        )\n\n        return parameter_settings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"def pre_process(_: str) -> T.Compose:\n    return T.Compose(\n        [\n            T.ToTensor(),\n            T.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711),\n            ),\n        ]\n    )\n\n\ndef aug(data_aug: str = \"image_net\") -> T.Compose:\n    transform = T.Compose(\n        [\n            T.ToPILImage(),\n            T.Resize(\n                size=(224, 224),\n                interpolation=T.InterpolationMode.BICUBIC,\n                antialias=True,\n            ),\n        ]\n    )\n    if data_aug == \"image_net\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n\n    elif data_aug == \"hca\":\n        aug8p3 = A.OneOf(\n            [\n                A.Sharpen(p=0.3),\n                A.ToGray(p=0.3),\n                A.CLAHE(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        blur = A.OneOf(\n            [\n                A.GaussianBlur(p=0.3),\n                A.MotionBlur(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        transform = A.Compose(\n            [\n                A.ShiftScaleRotate(\n                    rotate_limit=45,\n                    scale_limit=0.1,\n                    border_mode=cv2.BORDER_REFLECT,\n                    interpolation=cv2.INTER_CUBIC,\n                    p=0.5,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                aug8p3,\n                blur,\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.ElasticTransform(p=0.5),\n                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            ]\n        )\n    elif data_aug == \"aug_mix\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AugMix(),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n    elif data_aug == \"happy_whale\":\n        aug8p3 = A.OneOf(\n            [\n                A.Sharpen(p=0.3),\n                A.ToGray(p=0.3),\n                A.CLAHE(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        transform = A.Compose(\n            [\n                A.ShiftScaleRotate(\n                    rotate_limit=15,\n                    scale_limit=0.1,\n                    border_mode=cv2.BORDER_REFLECT,\n                    p=0.5,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                aug8p3,\n                A.HorizontalFlip(p=0.5),\n                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            ]\n        )\n\n    elif data_aug == \"cut_out\":\n        transform = A.Compose(\n            [\n                A.HorizontalFlip(p=0.5),\n                A.ImageCompression(quality_lower=99, quality_upper=100),\n                A.ShiftScaleRotate(\n                    shift_limit=0.2,\n                    scale_limit=0.2,\n                    rotate_limit=10,\n                    border_mode=cv2.BORDER_REFLECT,\n                    p=0.7,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                A.Cutout(\n                    max_h_size=int(224 * 0.4),\n                    max_w_size=int(224 * 0.4),\n                    num_holes=1,\n                    p=0.5,\n                ),\n            ]\n        )\n    elif data_aug == \"clip\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.RandomResizedCrop(\n                    size=(224, 224),\n                    scale=(0.9, 1.0),\n                    ratio=(0.75, 1.3333),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n    elif data_aug == \"clip+image_net\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n                T.RandomResizedCrop(\n                    size=(224, 224),\n                    scale=(0.9, 1.0),\n                    ratio=(0.75, 1.3333),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n\n    return transform\n\n\ndef read_image_cv2(f_name: str, gray_scale: bool = False) -> np.ndarray:\n    img = cv2.imread(\n        f_name, cv2.IMREAD_ANYCOLOR if not gray_scale else cv2.IMREAD_GRAYSCALE\n    )\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\n\ndef class_balancing(df: pd.DataFrame) -> pd.DataFrame:\n    counts = df.class_label.value_counts().to_dict()\n    max_label = max(list(counts.items()), key=lambda x: x[1])\n\n    for key, value in counts.items():\n        if key == max_label[0]:\n            continue\n\n        df_label = df[df.class_label == key].sample(\n            n=max_label[1] - value, replace=True\n        )\n        df = pd.concat([df, df_label])\n\n    return df\n\n\nclass SimpleClassificationDataset(Dataset):\n    def __init__(\n        self,\n        annotations_df: pd.DataFrame,\n        img_dir: str,\n        class_dict: dict,\n        transform: Optional[T.Compose] = None,\n        data_augment: Optional[Union[T.Compose, A.Compose]] = None,\n        class_balance: bool = True,\n    ):\n        self.df = annotations_df\n        if class_balance:\n            self.df = class_balancing(annotations_df)\n\n        self.img_dir = img_dir\n        self.class_dict = class_dict\n        self.transform = transform\n        self.data_augment = data_augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        cv2.setNumThreads(6)\n\n        f_name, _, _, x_tl, y_tl, x_br, y_br, label = self.df.iloc[idx]\n\n        img = read_image_cv2(os.path.join(self.img_dir, f_name))\n        img_ = img[y_tl:y_br, x_tl:x_br, :]\n        if img_.shape[0] * img_.shape[1] != 0:\n            img = img_\n\n        if self.data_augment:\n            if isinstance(self.data_augment, A.Compose):\n                img = self.data_augment(image=img)[\"image\"]\n            else:\n                img = self.data_augment(img)\n\n        if self.transform:\n            img = self.transform(img)\n\n        if self.class_dict:\n            label = self.class_dict[label]\n        return {\"img\": img, \"label\": label}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"def f1(y_true: th.Tensor, y_pred: th.Tensor):\n    y_pred = th.round(y_pred)\n    tp = th.sum((y_true * y_pred).float(), dim=0)\n    tn = th.sum(((1 - y_true) * (1 - y_pred)).float(), dim=0)\n    fp = th.sum(((1 - y_true) * y_pred).float(), dim=0)\n    fn = th.sum((y_true * (1 - y_pred)).float(), dim=0)\n\n    p = tp / (tp + fp + 1e-7)\n    r = tp / (tp + fn + 1e-7)\n\n    f1 = 2 * p * r / (p + r + 1e-7)\n    f1 = th.where(th.isnan(f1), th.zeros_like(f1), f1)\n    return th.mean(f1)\n\n\ndef f1_loss(y_true: th.Tensor, y_pred: th.Tensor):\n    tp = th.sum((y_true * y_pred).float(), dim=0)\n    tn = th.sum(((1 - y_true) * (1 - y_pred)).float(), dim=0)\n    fp = th.sum(((1 - y_true) * y_pred).float(), dim=0)\n    fn = th.sum((y_true * (1 - y_pred)).float(), dim=0)\n\n    p = tp / (tp + fp + 1e-7)\n    r = tp / (tp + fn + 1e-7)\n\n    f1 = 2 * p * r / (p + r + 1e-7)\n    f1 = th.where(th.isnan(f1), th.zeros_like(f1), f1)\n    return 1 - th.mean(f1)\n\n\ndef accuracy(y1: th.Tensor, y2: th.Tensor):\n    y1_argmax = y1.argmax(dim=1)\n    y2_argmax = y2.argmax(dim=1)\n\n    correct_sum = th.sum(y1_argmax == y2_argmax)\n    return correct_sum / len(y1)\n\n\nclass MosquitoClassifier(pl.LightningModule):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        dataset: str = \"datacomp_xl_s13b_b90k\",\n        freeze_backbones: bool = False,\n        head_version: int = 0,\n        warm_up_steps: int = 2000,\n        bs: int = 64,\n        data_aug: str = \"\",\n        loss_func: str = \"ce\",\n        epochs: int = 5,\n        class_weight = th.ones(6)\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.cls = CLIPClassifier(n_classes, model_name, dataset, head_version)\n        if freeze_backbones:\n            self.freezebackbone()\n\n        self.scheduler = None\n        self.n_classes = n_classes\n        self.warm_up_steps = warm_up_steps\n        self.loss_func = loss_func\n        self.class_weight = class_weight.cuda()\n\n        self.val_labels_t = []\n        self.val_labels_p = []\n\n        self.train_labels_t = []\n        self.train_labels_p = []\n\n    def freezebackbone(self) -> None:\n        for param in self.cls.backbone.parameters():\n            param.requires_grad = False\n\n    def forward(self, x: th.Tensor) -> th.Tensor:\n        return self.cls(x)\n\n    def lr_schedulers(self):\n        # over-write this shit\n        return self.scheduler\n\n    def configure_optimizers(self):\n        optimizer = th.optim.AdamW(self.cls.get_learnable_params())\n        if USE_LINEAR_SCHEDULE:\n            self.scheduler = get_linear_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=self.warm_up_steps,\n                num_training_steps=NUM_TRAINING_STEPS,  # not sure what to set\n            )\n        return optimizer\n\n    def compute_loss(self, label_t: th.Tensor, label_p: th.Tensor) -> th.Tensor:\n        if self.loss_func == \"f1\":\n            label_loss = f1_loss(label_t, th.nn.functional.softmax(label_p, dim=1))\n        elif self.loss_func == \"ce+f1\":\n            label_loss = f1_loss(\n                label_t, th.nn.functional.softmax(label_p, dim=1)\n            ) + nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING, weight=self.class_weight)(label_p, label_t)\n        else:\n            label_loss = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING, weight=self.class_weight)(label_p, label_t)\n\n        return label_loss\n\n    def training_step(self, train_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            train_batch[\"img\"],\n            train_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.train_labels_t.append(label_t.detach().cpu())\n        self.train_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"train_loss\", label_loss, prog_bar=True)\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n\n        return label_loss\n\n    def on_train_epoch_end(self) -> None:\n        label_p = th.concatenate(self.train_labels_p)\n        label_t = th.concatenate(self.train_labels_t)\n\n        self.log_dict(\n            {\n                \"train_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.train_labels_t = []\n        self.train_labels_p = []\n\n    def validation_step(self, val_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            val_batch[\"img\"],\n            val_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.val_labels_t.append(label_t.detach().cpu())\n        self.val_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"val_loss\", label_loss, prog_bar=True)\n\n        return label_loss\n\n    def on_validation_epoch_end(self):\n        label_p = th.concatenate(self.val_labels_p)\n        label_t = th.concatenate(self.val_labels_t)\n\n        self.log_dict(\n            {\n                \"val_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.val_labels_t = []\n        self.val_labels_p = []\n\n    def on_epoch_end(self):\n        opt = self.optimizers(use_pl_optimizer=True)\n        self.log(\"lr\", opt.param_groups[0][\"lr\"])\n        \nclass EMA(nn.Module):\n    \"\"\"Model Exponential Moving Average V2 from timm\"\"\"\n\n    def __init__(self, model: nn.Module, decay: float = 0.9999):\n        super(EMA, self).__init__()\n        # make a copy of the model for accumulating moving average of weights\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n\n    def _update(self, model: nn.Module, update_fn):\n        with th.no_grad():\n            for ema_v, model_v in zip(\n                self.module.state_dict().values(), model.state_dict().values()\n            ):\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(\n            model, update_fn=lambda e, m: self.decay * e + (1.0 - self.decay) * m\n        )\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n\n\nclass MosquitoClassifier(pl.LightningModule):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        dataset: str = None,\n        head_version: int = 0,\n        use_ema: bool = False,\n    ):\n        super().__init__()\n        if dataset == \"imagenet\":\n            self.cls = build_covnext(model_name, n_classes)\n\n        else:\n            self.cls = CLIPClassifier(n_classes, model_name, dataset, head_version)\n\n        self.use_ema = use_ema\n        if use_ema:\n            self.ema = EMA(self.cls, decay=0.995)\n            \n        self.freezebackbone()\n        self.scheduler = None\n        self.n_classes = n_classes\n        self.warm_up_steps = 1000\n        self.loss_func = \"ce+f1\"\n        self.class_weight = th.ones(6).cuda()\n        self.val_labels_t = []\n        self.val_labels_p = []\n        self.train_labels_t = []\n        self.train_labels_p = []\n\n    def forward(self, x: th.Tensor) -> th.Tensor:\n        if self.use_ema and not self.training:\n            print(\"Using EMA...\")\n            return self.ema.module(x)\n        return self.cls(x)\n    \n    def freezebackbone(self) -> None:\n        for param in self.cls.backbone.parameters():\n            param.requires_grad = False\n\n    def lr_schedulers(self):\n        # over-write this shit\n        return self.scheduler\n\n    def configure_optimizers(self):\n        optimizer = th.optim.AdamW(self.cls.get_learnable_params())\n        if USE_LINEAR_SCHEDULE:\n            self.scheduler = get_linear_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=self.warm_up_steps,\n                num_training_steps=NUM_TRAINING_STEPS,  # not sure what to set\n            )\n        return optimizer\n    \n    def compute_loss(self, label_t: th.Tensor, label_p: th.Tensor) -> th.Tensor:\n        if self.loss_func == \"f1\":\n            label_loss = f1_loss(label_t, th.nn.functional.softmax(label_p, dim=1))\n        elif self.loss_func == \"ce+f1\":\n            label_loss = f1_loss(\n                label_t, th.nn.functional.softmax(label_p, dim=1)\n            ) + nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING, weight=self.class_weight)(label_p, label_t)\n        else:\n            label_loss = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING, weight=self.class_weight)(label_p, label_t)\n\n        return label_loss\n\n    def training_step(self, train_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            train_batch[\"img\"],\n            train_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.train_labels_t.append(label_t.detach().cpu())\n        self.train_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"train_loss\", label_loss, prog_bar=True)\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n\n        return label_loss\n    \n    def on_train_epoch_end(self) -> None:\n        label_p = th.concatenate(self.train_labels_p)\n        label_t = th.concatenate(self.train_labels_t)\n\n        self.log_dict(\n            {\n                \"train_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.train_labels_t = []\n        self.train_labels_p = []\n    \n    def validation_step(self, val_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            val_batch[\"img\"],\n            val_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.val_labels_t.append(label_t.detach().cpu())\n        self.val_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"val_loss\", label_loss, prog_bar=True)\n\n        return label_loss\n    \n    def on_validation_epoch_end(self):\n        label_p = th.concatenate(self.val_labels_p)\n        label_t = th.concatenate(self.val_labels_t)\n\n        self.log_dict(\n            {\n                \"val_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.val_labels_t = []\n        self.val_labels_p = []\n\n    def on_epoch_end(self):\n        opt = self.optimizers(use_pl_optimizer=True)\n        self.log(\"lr\", opt.param_groups[0][\"lr\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiments","metadata":{}},{"cell_type":"code","source":"def _default_callbacks() -> List[Callback]:\n    return [\n        ModelCheckpoint(\n            monitor=\"val_f1_score\",\n            mode=\"max\",\n            save_top_k=3,\n            save_last=False,\n            filename=\"{epoch}-{val_loss}-{val_f1_score}-{val_multiclass_accuracy}\",\n            save_weights_only=True\n        ),\n        \n    ]\n\n\nCLASS_DICT = {\n    \"albopictus\": th.tensor([1, 0, 0, 0, 0, 0], dtype=th.float),\n    \"culex\": th.tensor([0, 1, 0, 0, 0, 0], dtype=th.float),\n    \"japonicus/koreicus\": th.tensor([0, 0, 1, 0, 0, 0], dtype=th.float),\n    \"culiseta\": th.tensor([0, 0, 0, 1, 0, 0], dtype=th.float),\n    \"anopheles\": th.tensor([0, 0, 0, 0, 1, 0], dtype=th.float),\n    \"aegypti\": th.tensor([0, 0, 0, 0, 0, 1], dtype=th.float),\n}\n\n\nclass ExperimentMosquitoClassifier:\n    def __init__(\n        self,\n        img_dir: str,\n        annotations_df: pd.DataFrame,\n        class_dict: Dict[str, th.Tensor] = CLASS_DICT,\n    ):\n        self.img_dir = img_dir\n        self.annotations_df = annotations_df\n        self.class_dict = class_dict\n\n    def get_dataloaders(\n        self,\n        train_df: pd.DataFrame,\n        val_df: pd.DataFrame,\n        model_name: str,\n        data_aug: str,\n        bs: int,\n    ) -> List[DataLoader]:\n        transform = pre_process(model_name)\n\n        train_dataset = SimpleClassificationDataset(\n            train_df,\n            self.img_dir,\n            self.class_dict,\n            transform,\n            aug(data_aug),\n            class_balance=False,\n        )\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=bs,\n            shuffle=True,\n            num_workers=4,\n            drop_last=True,\n        )\n\n        val_dataset = SimpleClassificationDataset(\n            val_df,\n            self.img_dir,\n            self.class_dict,\n            transform,\n            aug(\"resize\"),\n            class_balance=False,\n        )\n        val_dataloader = DataLoader(\n            val_dataset,\n            batch_size=bs,\n            shuffle=False,\n            num_workers=4,\n        )\n\n        return train_dataloader, val_dataloader\n\n    def run(\n        self,\n        model_name: str,\n        dataset: str,\n        bs: int,\n        head_version: int,\n        data_aug: str,\n        freeze_backbones: bool = False,\n        warm_up_steps: int = 2000,\n        epochs: int = 5,\n        create_callbacks: Callable[[], List[Callback]] = _default_callbacks,\n    ):\n        # Challenge data\n        annotations_challenge = self.annotations_df[self.annotations_df['img_fName'].str.contains(r\"train_\\d+.jpeg\")]\n        # annotations_challenge = self.annotations_df\n        \n        train_df, val_df = train_test_split(\n            annotations_challenge,\n            test_size=TEST_SIZE,\n            stratify=annotations_challenge[\"class_label\"],\n            random_state=200,\n        )\n        \n        if USE_ZENODO_DATA or USE_INATURALIST_DATA or USE_LUX:\n            train_df = pd.concat([train_df, self.annotations_df[~self.annotations_df['img_fName'].str.contains(r\"train_\\d+.jpeg\")]])\n        else:\n            train_df = class_balancing(train_df)\n        \n        if USE_CLASS_WEIGHTS:\n            class_weight = compute_class_weight(class_weight='balanced', classes=np.array(list(CLASS_DICT.keys())), y=train_df['class_label'].to_numpy())\n            class_weight = th.tensor(class_weight, dtype=th.float)\n        else:\n            class_weight = th.ones(6)\n        print(class_weight)\n        \n        print(train_df[\"class_label\"].value_counts())\n        print(val_df[\"class_label\"].value_counts())\n\n        train_dataloader, val_dataloader = self.get_dataloaders(\n            train_df, val_df, model_name, data_aug, bs\n        )\n\n        th.set_float32_matmul_precision(\"high\")\n        if not USE_PRETRAINED:\n            model = MosquitoClassifier(\n                model_name=model_name,\n                dataset=dataset,\n                freeze_backbones=freeze_backbones,\n                head_version=head_version,\n                warm_up_steps=warm_up_steps,\n                bs=bs,\n                data_aug=data_aug,\n                epochs=epochs,\n                class_weight=class_weight\n            )\n        else:\n            clip_model_path = \"/kaggle/input/mosquito-base-model/epoch11-val_loss0.7277861833572388-val_f1_score0.8598743081092834-val_multiclass_accuracy0.8671819567680359.ckpt\"\n            # model = MosquitoClassifier.load_from_checkpoint(\n            # clip_model_path, head_version=4, freeze_backbones=True, loss_function=\"ce+f1\", map_location=th.device(\"cuda\"))\n            model = MosquitoClassifier.load_from_checkpoint(clip_model_path)\n        trainer = pl.Trainer(\n            accelerator=\"gpu\",\n            precision=\"16-mixed\",\n            max_epochs=epochs,\n            logger=True,\n            deterministic=True,  # maybe we should add this\n            callbacks=create_callbacks(),\n        )\n\n        trainer.fit(\n            model=model,\n            train_dataloaders=train_dataloader,\n            val_dataloaders=val_dataloader,\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0_cleaned_yolo_best_annotations.csv\"\nannotations_df = pd.read_csv(annotations_csv)\nannotations_df[\"bbx_xtl\"] = annotations_df[\"bbx_xtl\"].astype(int)\nannotations_df[\"bbx_ytl\"] = annotations_df[\"bbx_ytl\"].astype(int)\nannotations_df[\"bbx_xbr\"] = annotations_df[\"bbx_xbr\"].astype(int)\nannotations_df[\"bbx_ybr\"] = annotations_df[\"bbx_ybr\"].astype(int)\n\nzenodo_img_dir = \"/kaggle/input/zenodo-images/zenodo/images/\"\nzenodo_annotations_csv = \"/kaggle/input/zenodo-images/zenodo/zenodo_annotations.csv\"\nif USE_ZENODO_DATA:\n    annotations_df['img_fName'] = img_dir+annotations_df['img_fName']\n    img_dir = '.'\n    zenodo_annotations_df = pd.read_csv(zenodo_annotations_csv)\n    zenodo_annotations_df['img_fName'] = zenodo_annotations_df['img_fName'].str.replace('zenodo_', '')\n    zenodo_annotations_df['img_fName'] = zenodo_img_dir + zenodo_annotations_df['img_fName']\n    if N_ZENODO_DATA:\n        zenodo_annotations_df = zenodo_annotations_df.sample(n=N_ZENODO_DATA, random_state=200)\n    annotations_df = pd.concat([annotations_df, zenodo_annotations_df])\n\ninat_img_dir = \"/kaggle/input/inaturalist-mosquito/inaturalist_crawler_data_all_cleaned_sub/\"\ninat_annotations_csv = inat_img_dir+\"inaturalist.csv\"\nif USE_INATURALIST_DATA:\n    annotations_df['img_fName'] = img_dir+annotations_df['img_fName']\n    img_dir = '.'\n    inat_annotations_df = pd.read_csv(inat_annotations_csv)\n    inat_annotations_df = inat_annotations_df[inat_annotations_df['class_label']!='culex']\n    inat_annotations_df = inat_annotations_df[inat_annotations_df['class_label']!='albopictus']\n    inat_annotations_df['img_fName'] = inat_img_dir + inat_annotations_df['img_fName']\n    annotations_df = pd.concat([annotations_df, inat_annotations_df])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lux_dir1 = \"/kaggle/input/lux-dataset-annotated/gbif-cropped/\"\nlux_dir2 = \"/kaggle/input/lux-dataset-annotated/inaturalist-six-cropped/\"\nlux_an_csv1 = \"/kaggle/input/lux-dataset-annotated/gbif-cropped/inaturalist.csv\"\nlux_an_csv2 = \"/kaggle/input/lux-dataset-annotated/inaturalist-six-cropped/inaturalist.csv\"\nif USE_LUX:\n    annotations_df['img_fName'] = img_dir+annotations_df['img_fName']\n    img_dir = '.'\n    lux1_annotations_df = pd.read_csv(lux_an_csv1)\n    lux1_annotations_df['img_fName'] = lux_dir1 + lux1_annotations_df['img_fName']\n    # lux1_annotations_df = lux1_annotations_df[lux1_annotations_df['class_label']!='culex']\n    # lux1_annotations_df = lux1_annotations_df[lux1_annotations_df['class_label']!='albopictus']\n    lux2_annotations_df = pd.read_csv(lux_an_csv2)\n    lux2_annotations_df['img_fName'] = lux_dir2 + lux2_annotations_df['img_fName']\n    lux2_annotations_df = lux2_annotations_df[lux2_annotations_df['class_label']!='culex']\n    lux2_annotations_df = lux2_annotations_df[lux2_annotations_df['class_label']!='albopictus']\n    annotations_df = pd.concat([annotations_df, lux1_annotations_df, lux2_annotations_df])\n    lux1_annotations_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(annotations_df[\"class_label\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n\nclass_dict = {\n    \"albopictus\": th.tensor([1, 0, 0, 0, 0, 0], dtype=th.long),\n    \"culex\": th.tensor([0, 1, 0, 0, 0, 0], dtype=th.long),\n    \"japonicus/koreicus\": th.tensor([0, 0, 1, 0, 0, 0], dtype=th.long),\n    \"culiseta\": th.tensor([0, 0, 0, 1, 0, 0], dtype=th.long),\n    \"anopheles\": th.tensor([0, 0, 0, 0, 1, 0], dtype=th.long),\n    \"aegypti\": th.tensor([0, 0, 0, 0, 0, 1], dtype=th.long),\n}\n\ntransform = pre_process(\"\")\n\ndata_augmentation = aug(\"hca\")\n\nds = SimpleClassificationDataset(\n    annotations_df=annotations_df,\n    img_dir=img_dir,\n    class_dict=class_dict,\n    transform=transform,\n    data_augment=data_augmentation,\n)\nfor i in range(10):\n    res = ds[i]\n    img = res[\"img\"]\n\n    img_bbox = th.tensor(\n        255 * (img - img.min()) / (img.max() - img.min()), dtype=th.uint8\n    )\n    show(img_bbox)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = ExperimentMosquitoClassifier(img_dir, annotations_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp.run(\n    model_name = MODEL_NAME,\n    dataset = PRETRAIN_DATASET,\n    bs = BATCH_SIZE,\n    head_version = HEAD_NUMBER,\n    data_aug = AUGMENTATION,\n    freeze_backbones = FREEZE_BACKBONE,\n    warm_up_steps = WARMUP_STEPS,\n    epochs = EPOCHS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Soup","metadata":{}},{"cell_type":"code","source":"class MosquitoClassifier(pl.LightningModule):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        dataset: str = None,\n        head_version: int = 0,\n    ):\n        super().__init__()\n        self.cls = CLIPClassifier(n_classes, model_name, dataset, head_version)\n\n    def forward(self, x: th.Tensor) -> th.Tensor:\n        return self.cls(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef model_soup(path_list: List[str], save_path: str):\n    # Load models weights\n    weight_list = []\n\n    for path in path_list:\n        print(f\"Loading {path}...\")\n        model = MosquitoClassifier.load_from_checkpoint(\n            path, map_location=torch.device(\"cpu\")\n        )\n        weight_list.append(model.state_dict())\n\n    # Average weights\n    state_dict = {}\n\n    for k in weight_list[0]:\n        try:\n            w = torch.stack([v[k] for v in weight_list]).mean(0)\n            print(f\"Mean: Layer {k}...\")\n        except:\n            w = torch.stack([v[k] for v in weight_list]).sum(0)\n            print(f\"Sum: Layer {k}...\")\n\n        state_dict[k] = w\n    model.load_state_dict(state_dict)\n\n    trainer = pl.Trainer()\n    trainer.strategy.connect(model)\n    trainer.save_checkpoint(save_path)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = [\n    '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=5-val_loss=0.7609413266181946-val_f1_score=0.8209875226020813-val_multiclass_accuracy=0.7888965606689453.ckpt',\n    '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=7-val_loss=0.7274188995361328-val_f1_score=0.8357369303703308-val_multiclass_accuracy=0.8175098896026611.ckpt' \n]\n\nmodel_soup(model_names, \"/kaggle/working/models/version_4_averaged.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = [\n    '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=5-val_loss=0.7001691460609436-val_f1_score=0.8165072798728943-val_multiclass_accuracy=0.7728392481803894.ckpt',\n    '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=7-val_loss=0.7274188995361328-val_f1_score=0.8357369303703308-val_multiclass_accuracy=0.8175098896026611.ckpt' \n]\n\nmodel_soup(model_names, \"/kaggle/working/models/version_34_averaged.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bounding Boxes","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nfrom pathlib import Path\nimport cv2\nfrom timeit import default_timer as timer\nimport timeit\nimport time\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/')\nowl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/').cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[\"a photo of a mosquito\"]]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = time.time()\nwith torch.no_grad():\n    inputs = owl_processor(text=texts, images=image, padding='do_not_pad', return_tensors=\"pt\")\n    outputs = owl_model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n\n    # Convert outputs (bounding boxes and class logits) to COCO API\n    results = owl_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.01)\ne = time.time()\nprint(\"OWL Time \", 1000 * (e - s), \"ms\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef locate_object():\n    # inputs = owl_processor(text=texts, images=image, padding='do_not_pad', return_tensors=\"pt\")\n    inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\")\n    outputs = owl_model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n\n    # Convert outputs (bounding boxes and class logits) to COCO API\n    results = owl_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.01)\n\ntimeit.timeit(locate_object, number=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    text_tokens = owl_processor(text=texts, return_tensors=\"pt\")\n\n@torch.no_grad()\ndef locate_object():\n    # inputs = owl_processor(text=texts, images=image, padding='do_not_pad', return_tensors=\"pt\")\n    # inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\")\n    outputs = owl_model(**text_tokens, **owl_processor.image_processor(images=image, return_tensors=\"pt\", size=768))\n    target_sizes = torch.Tensor([image.size[::-1]])\n\n    # Convert outputs (bounding boxes and class logits) to COCO API\n    results = owl_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.01)\n\ntimeit.timeit(locate_object, number=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import YolosImageProcessor, YolosForObjectDetection\nfrom PIL import Image\nimport torch\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n\n\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0.csv\"\nroot_images = os.path.join(img_dir)\n\nall_images = os.listdir(root_images)\nprint(f\"Total images: {len(all_images)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image.size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_image_file = os.path.join(root_images, all_images[3])\nimage = Image.open(original_image_file)\n\nwith torch.no_grad():\n    inputs = image_processor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n\n\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )\nresults['iou'] = np.array([bb_intersection_over_union(box.tolist(), [0, 0, *image.size]) for box in results[\"boxes\"]])\nprint(results['iou'])\nresults[\"scores\"] = results[\"scores\"][results['iou']<0.95]\nresults[\"boxes\"] = results[\"boxes\"][results['iou']<0.95]\n    \nbest_score_index = np.argmax(results[\"scores\"].numpy())\nbox = results[\"boxes\"][best_score_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure and axes\nfig, ax = plt.subplots()\n\n# Display the image\nax.imshow(image)\n\n# Create a Rectangle patch\nbox = [round(i, 2) for i in box.tolist()]\nrect_owl = patches.Rectangle(box[:2], box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none', label='Owl-ViT')\nax.add_patch(rect_owl)\nplt.legend()\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/')\nowl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/').cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_images = os.path.join(img_dir)\n\nall_images = os.listdir(root_images)\nprint(f\"Total images: {len(all_images)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = []\nfor original_image in tqdm(all_images):\n    original_image_file = os.path.join(root_images, original_image)\n    image = Image.open(original_image_file)\n    with torch.no_grad():\n        texts = [[\"a photo of a mosquito\"]]\n        inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\").to('cuda')\n        outputs = owl_model(**inputs)\n        # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n        target_sizes = torch.Tensor([image.size[::-1]]).to('cuda')\n\n        # Convert outputs (bounding boxes and class logits) to COCO API\n        results = owl_processor.post_process_object_detection(outputs=outputs, \n                                                            target_sizes=target_sizes, \n                                                            threshold=0.01)\n        \n        i = 0  # Retrieve predictions for the first image for the corresponding text queries\n        text = texts[i]\n        boxes, scores, labels = results[i][\"boxes\"].cpu().numpy(), results[i][\"scores\"].cpu().numpy(), results[i][\"labels\"].cpu().numpy()\n        best_score_index = np.argmax(scores)\n        boxes, scores, labels = boxes[best_score_index], scores[best_score_index], labels[best_score_index]\n        row = [original_image, boxes[0], boxes[1], boxes[2], boxes[3]]\n        rows.append(row)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(rows, columns=[\"img_fName\", \"bbx_xtl\", \"bbx_ytl\", \"bbx_xbr\", \"bbx_ybr\"])\ndf.to_csv('/kaggle/working/owl_vit_image_bboxes.csv', index=False)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df = pd.read_csv(annotations_csv)\nannotations_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged = annotations_df.merge(df, on='img_fName', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['bbox_true'] = df_merged.apply(lambda x: [x.bbx_xtl_x, x.bbx_ytl_x, x.bbx_xbr_x, x.bbx_ybr_x], axis=1)\ndf_merged['bbox_owl'] = df_merged.apply(lambda x: [x.bbx_xtl_y, x.bbx_ytl_y, x.bbx_xbr_y, x.bbx_ybr_y], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['IoU'] = df_merged.apply(lambda x: bb_intersection_over_union(x.bbox_true, x.bbox_owl), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['IoU'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged[df_merged['IoU']<0.1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 10224\n\nim = Image.open(img_dir+df_merged['img_fName'].iloc[idx])\n\n# Create figure and axes\nfig, ax = plt.subplots()\n\n# Display the image\nax.imshow(im)\n\n# Create a Rectangle patch\nrect_baseline = patches.Rectangle(\n    df_merged['bbox_true'].iloc[idx][:2], \n    df_merged['bbox_true'].iloc[idx][2]-df_merged['bbox_true'].iloc[idx][0],\n    df_merged['bbox_true'].iloc[idx][3]-df_merged['bbox_true'].iloc[idx][1], linewidth=1, edgecolor='b', facecolor='none', label='ground truth')\n# Add the patch to the Axes\nax.add_patch(rect_baseline)\n\n# Create a Rectangle patch\nrect_owl = patches.Rectangle(\n    df_merged['bbox_owl'].iloc[idx][:2], \n    df_merged['bbox_owl'].iloc[idx][2]-df_merged['bbox_owl'].iloc[idx][0],\n    df_merged['bbox_owl'].iloc[idx][3]-df_merged['bbox_owl'].iloc[idx][1], linewidth=1, edgecolor='r', facecolor='none', label='Owl-ViT')\n# Add the patch to the Axes\nax.add_patch(rect_owl)\nplt.legend()\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}