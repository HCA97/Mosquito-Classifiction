{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/working/lightning_logs/version_4/checkpoints/","metadata":{"execution":{"iopub.status.busy":"2023-09-28T19:26:52.891285Z","iopub.execute_input":"2023-09-28T19:26:52.891795Z","iopub.status.idle":"2023-09-28T19:26:54.030293Z","shell.execute_reply.started":"2023-09-28T19:26:52.891754Z","shell.execute_reply":"2023-09-28T19:26:54.029091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf /kaggle/working/lightning_logs/version_1/checkpoints/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'lightning_logs/version_4/checkpoints/epoch=7-val_loss=0.7274188995361328-val_f1_score=0.8357369303703308-val_multiclass_accuracy=0.8175098896026611.ckpt')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T19:28:12.040307Z","iopub.execute_input":"2023-09-28T19:28:12.040766Z","iopub.status.idle":"2023-09-28T19:28:12.050991Z","shell.execute_reply.started":"2023-09-28T19:28:12.040731Z","shell.execute_reply":"2023-09-28T19:28:12.049724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torcheval","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:17:44.325557Z","iopub.execute_input":"2023-09-28T09:17:44.325979Z","iopub.status.idle":"2023-09-28T09:17:57.119109Z","shell.execute_reply.started":"2023-09-28T09:17:44.325927Z","shell.execute_reply":"2023-09-28T09:17:57.117896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:17:57.121728Z","iopub.execute_input":"2023-09-28T09:17:57.123240Z","iopub.status.idle":"2023-09-28T09:18:08.696652Z","shell.execute_reply.started":"2023-09-28T09:17:57.123199Z","shell.execute_reply":"2023-09-28T09:18:08.695492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict, Callable, Optional, Union, Tuple, Any\n\nimport os\nimport logging\n\nimport random\n\nimport torch as th\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, Callback\nfrom pytorch_lightning.utilities.types import STEP_OUTPUT\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy\nfrom transformers import get_linear_schedule_with_warmup\n\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as T\nimport albumentations as A\n\nimport open_clip","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:08.699133Z","iopub.execute_input":"2023-09-28T09:18:08.700157Z","iopub.status.idle":"2023-09-28T09:18:25.058976Z","shell.execute_reply.started":"2023-09-28T09:18:08.700117Z","shell.execute_reply":"2023-09-28T09:18:25.057926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = \"ViT-L-14\"\nPRETRAIN_DATASET = \"datacomp_xl_s13b_b90k\"\nBATCH_SIZE = 16\nHEAD_NUMBER = 4\nAUGMENTATION = \"hca\"\nFREEZE_BACKBONE = False\nWARMUP_STEPS = 1000\nEPOCHS = 8\nLABEL_SMOOTHING = 0.1\nDROPOUT_RATE = 0.5\nUSE_LINEAR_SCHEDULE = True\nUSE_LAYER_NORM = True","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.061713Z","iopub.execute_input":"2023-09-28T09:18:25.062182Z","iopub.status.idle":"2023-09-28T09:18:25.068583Z","shell.execute_reply.started":"2023-09-28T09:18:25.062144Z","shell.execute_reply":"2023-09-28T09:18:25.066991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class HeadV1(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(p=DROPOUT_RATE),\n            nn.LeakyReLU(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV4(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n        if USE_LAYER_NORM:\n            norm_layer = nn.LayerNorm(f_in)\n        else:\n            norm_layer = nn.BatchNorm1d(f_in)\n        self.label = nn.Sequential(\n            norm_layer,\n            nn.Dropout1d(p=DROPOUT_RATE),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV3(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Linear(f_in, f_in, bias=False),\n            nn.BatchNorm1d(f_in),\n            nn.LeakyReLU(),\n            nn.Dropout1d(p=DROPOUT_RATE),\n            nn.Linear(f_in, f_in, bias=False),\n            nn.BatchNorm1d(f_in),\n            nn.LeakyReLU(),\n            nn.Dropout1d(p=DROPOUT_RATE),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass CLIPClassifier(nn.Module):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        data: str = \"datacomp_xl_s13b_b90k\",\n        head_version: int = 1,\n    ):\n        super().__init__()\n        self.backbone = open_clip.create_model_and_transforms(\n            model_name, pretrained=data\n        )[0].visual\n\n        if model_name == \"ViT-L-14\":\n            self.n = 768\n            self.lrs = dict(\n                back_lrs={\"8\": 1.25e-6, \"16\": 2.5e-6, \"20\": 5e-6, \"24\": 10e-6},\n                back_wd=1e-3,\n                hd_lr=3e-4,\n                hd_wd=1e-5,\n            )\n        elif model_name == \"ViT-H-14\":\n            self.n = 1024\n            self.lrs = {\n                \"back_lrs\": {\"10\": 1.25e-6, \"20\": 2.5e-6, \"26\": 5e-6, \"32\": 10e-6},\n                \"back_wd\": 1e-3,\n                \"hd_lr\": 3e-4,\n                \"hd_wd\": 1e-5,\n            }\n        elif model_name == \"ViT-B-16\":\n            self.n = 512\n            self.lrs = {\n                \"back_lrs\": {\"1\": 2.5e-6, \"7\": 5e-6, \"12\": 10e-6},\n                \"back_wd\": 1e-3,\n                \"hd_lr\": 3e-4,\n                \"hd_wd\": 1e-5,\n            }\n        else:\n            raise ValueError\n\n        if head_version == 3:\n            self.label = HeadV3(n_classes, self.n)\n        elif head_version == 4:\n            self.label = HeadV4(n_classes, self.n)\n        else:\n            self.label = HeadV1(n_classes, self.n)\n\n        self.n_classes = n_classes\n\n    def forward(self, x: th.tensor) -> th.tensor:\n        x = self.backbone(x)\n        return self.label(x)\n\n    def get_parameter_section(self, parameters, lr=None, wd=None):\n        # https://github.com/IvanAer/G-Universal-CLIP\n        parameter_settings = []\n\n        lr_is_dict = isinstance(lr, dict)\n        wd_is_dict = isinstance(wd, dict)\n\n        layer_no = None\n        for n, p in parameters:\n            for split in n.split(\".\"):\n                if split.isnumeric():\n                    layer_no = int(split)\n\n            if not layer_no:\n                layer_no = 0\n\n            if lr_is_dict:\n                for k, v in lr.items():\n                    if layer_no < int(k):\n                        temp_lr = v\n                        break\n            else:\n                temp_lr = lr\n\n            if wd_is_dict:\n                for k, v in wd.items():\n                    if layer_no < int(k):\n                        temp_wd = v\n                        break\n            else:\n                temp_wd = wd\n\n            parameter_setting = {\"params\": p, \"lr\": temp_lr, \"weight_decay\": temp_wd}\n            parameter_settings.append(parameter_setting)\n        return parameter_settings\n\n    def get_learnable_params(self) -> list:\n        back_lrs = self.lrs[\"back_lrs\"]\n        back_wd = self.lrs[\"back_wd\"]\n        hd_lr = self.lrs[\"hd_lr\"]\n        hd_wd = self.lrs[\"hd_wd\"]\n\n        parameter_settings = []\n\n        if back_lrs and back_wd:\n            parameter_settings.extend(\n                self.get_parameter_section(\n                    [(n, p) for n, p in self.backbone.named_parameters()],\n                    lr=back_lrs,\n                    wd=back_wd,\n                )\n            )\n\n        parameter_settings.extend(\n            self.get_parameter_section(\n                [(n, p) for n, p in self.label.named_parameters()], lr=hd_lr, wd=hd_wd\n            )\n        )\n\n        return parameter_settings","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.070249Z","iopub.execute_input":"2023-09-28T09:18:25.070604Z","iopub.status.idle":"2023-09-28T09:18:25.098291Z","shell.execute_reply.started":"2023-09-28T09:18:25.070571Z","shell.execute_reply":"2023-09-28T09:18:25.097292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"def pre_process(_: str) -> T.Compose:\n    return T.Compose(\n        [\n            T.ToTensor(),\n            T.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711),\n            ),\n        ]\n    )\n\n\ndef aug(data_aug: str = \"image_net\") -> T.Compose:\n    transform = T.Compose(\n        [\n            T.ToPILImage(),\n            T.Resize(\n                size=(224, 224),\n                interpolation=T.InterpolationMode.BICUBIC,\n                antialias=True,\n            ),\n        ]\n    )\n    if data_aug == \"image_net\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n\n    elif data_aug == \"hca\":\n        aug8p3 = A.OneOf(\n            [\n                A.Sharpen(p=0.3),\n                A.ToGray(p=0.3),\n                A.CLAHE(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        blur = A.OneOf(\n            [\n                A.GaussianBlur(p=0.3),\n                A.MotionBlur(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        transform = A.Compose(\n            [\n                A.ShiftScaleRotate(\n                    rotate_limit=45,\n                    scale_limit=0.1,\n                    border_mode=cv2.BORDER_REFLECT,\n                    interpolation=cv2.INTER_CUBIC,\n                    p=0.5,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                aug8p3,\n                blur,\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.ElasticTransform(p=0.5),\n                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            ]\n        )\n    elif data_aug == \"aug_mix\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AugMix(),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n    elif data_aug == \"happy_whale\":\n        aug8p3 = A.OneOf(\n            [\n                A.Sharpen(p=0.3),\n                A.ToGray(p=0.3),\n                A.CLAHE(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        transform = A.Compose(\n            [\n                A.ShiftScaleRotate(\n                    rotate_limit=15,\n                    scale_limit=0.1,\n                    border_mode=cv2.BORDER_REFLECT,\n                    p=0.5,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                aug8p3,\n                A.HorizontalFlip(p=0.5),\n                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            ]\n        )\n\n    elif data_aug == \"cut_out\":\n        transform = A.Compose(\n            [\n                A.HorizontalFlip(p=0.5),\n                A.ImageCompression(quality_lower=99, quality_upper=100),\n                A.ShiftScaleRotate(\n                    shift_limit=0.2,\n                    scale_limit=0.2,\n                    rotate_limit=10,\n                    border_mode=cv2.BORDER_REFLECT,\n                    p=0.7,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                A.Cutout(\n                    max_h_size=int(224 * 0.4),\n                    max_w_size=int(224 * 0.4),\n                    num_holes=1,\n                    p=0.5,\n                ),\n            ]\n        )\n    elif data_aug == \"clip\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.RandomResizedCrop(\n                    size=(224, 224),\n                    scale=(0.9, 1.0),\n                    ratio=(0.75, 1.3333),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n    elif data_aug == \"clip+image_net\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n                T.RandomResizedCrop(\n                    size=(224, 224),\n                    scale=(0.9, 1.0),\n                    ratio=(0.75, 1.3333),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n\n    return transform\n\n\ndef read_image_cv2(f_name: str, gray_scale: bool = False) -> np.ndarray:\n    img = cv2.imread(\n        f_name, cv2.IMREAD_ANYCOLOR if not gray_scale else cv2.IMREAD_GRAYSCALE\n    )\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\n\ndef class_balancing(df: pd.DataFrame) -> pd.DataFrame:\n    counts = df.class_label.value_counts().to_dict()\n    max_label = max(list(counts.items()), key=lambda x: x[1])\n\n    for key, value in counts.items():\n        if key == max_label[0]:\n            continue\n\n        df_label = df[df.class_label == key].sample(\n            n=max_label[1] - value, replace=True\n        )\n        df = pd.concat([df, df_label])\n\n    return df\n\n\nclass SimpleClassificationDataset(Dataset):\n    def __init__(\n        self,\n        annotations_df: pd.DataFrame,\n        img_dir: str,\n        class_dict: dict,\n        transform: Optional[T.Compose] = None,\n        data_augment: Optional[Union[T.Compose, A.Compose]] = None,\n        class_balance: bool = True,\n    ):\n        self.df = annotations_df\n        if class_balance:\n            self.df = class_balancing(annotations_df)\n\n        self.img_dir = img_dir\n        self.class_dict = class_dict\n        self.transform = transform\n        self.data_augment = data_augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        cv2.setNumThreads(6)\n\n        f_name, _, _, x_tl, y_tl, x_br, y_br, label = self.df.iloc[idx]\n\n        img = read_image_cv2(os.path.join(self.img_dir, f_name))\n        img_ = img[y_tl:y_br, x_tl:x_br, :]\n        if img_.shape[0] * img_.shape[1] != 0:\n            img = img_\n\n        if self.data_augment:\n            if isinstance(self.data_augment, A.Compose):\n                img = self.data_augment(image=img)[\"image\"]\n            else:\n                img = self.data_augment(img)\n\n        if self.transform:\n            img = self.transform(img)\n\n        if self.class_dict:\n            label = self.class_dict[label]\n        return {\"img\": img, \"label\": label}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.099681Z","iopub.execute_input":"2023-09-28T09:18:25.100198Z","iopub.status.idle":"2023-09-28T09:18:25.134229Z","shell.execute_reply.started":"2023-09-28T09:18:25.100164Z","shell.execute_reply":"2023-09-28T09:18:25.133009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"def f1(y_true: th.Tensor, y_pred: th.Tensor):\n    y_pred = th.round(y_pred)\n    tp = th.sum((y_true * y_pred).float(), dim=0)\n    tn = th.sum(((1 - y_true) * (1 - y_pred)).float(), dim=0)\n    fp = th.sum(((1 - y_true) * y_pred).float(), dim=0)\n    fn = th.sum((y_true * (1 - y_pred)).float(), dim=0)\n\n    p = tp / (tp + fp + 1e-7)\n    r = tp / (tp + fn + 1e-7)\n\n    f1 = 2 * p * r / (p + r + 1e-7)\n    f1 = th.where(th.isnan(f1), th.zeros_like(f1), f1)\n    return th.mean(f1)\n\n\ndef f1_loss(y_true: th.Tensor, y_pred: th.Tensor):\n    tp = th.sum((y_true * y_pred).float(), dim=0)\n    tn = th.sum(((1 - y_true) * (1 - y_pred)).float(), dim=0)\n    fp = th.sum(((1 - y_true) * y_pred).float(), dim=0)\n    fn = th.sum((y_true * (1 - y_pred)).float(), dim=0)\n\n    p = tp / (tp + fp + 1e-7)\n    r = tp / (tp + fn + 1e-7)\n\n    f1 = 2 * p * r / (p + r + 1e-7)\n    f1 = th.where(th.isnan(f1), th.zeros_like(f1), f1)\n    return 1 - th.mean(f1)\n\n\ndef accuracy(y1: th.Tensor, y2: th.Tensor):\n    y1_argmax = y1.argmax(dim=1)\n    y2_argmax = y2.argmax(dim=1)\n\n    correct_sum = th.sum(y1_argmax == y2_argmax)\n    return correct_sum / len(y1)\n\n\nclass MosquitoClassifier(pl.LightningModule):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        dataset: str = \"datacomp_xl_s13b_b90k\",\n        freeze_backbones: bool = False,\n        head_version: int = 0,\n        warm_up_steps: int = 2000,\n        bs: int = 64,\n        data_aug: str = \"\",\n        loss_func: str = \"ce\",\n        epochs: int = 5,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.cls = CLIPClassifier(n_classes, model_name, dataset, head_version)\n        if freeze_backbones:\n            self.freezebackbone()\n\n        self.scheduler = None\n        self.n_classes = n_classes\n        self.warm_up_steps = warm_up_steps\n        self.loss_func = loss_func\n\n        self.val_labels_t = []\n        self.val_labels_p = []\n\n        self.train_labels_t = []\n        self.train_labels_p = []\n\n    def freezebackbone(self) -> None:\n        for param in self.cls.backbone.parameters():\n            param.requires_grad = False\n\n    def forward(self, x: th.Tensor) -> th.Tensor:\n        return self.cls(x)\n\n    def lr_schedulers(self):\n        # over-write this shit\n        return self.scheduler\n\n    def configure_optimizers(self):\n        optimizer = th.optim.AdamW(self.cls.get_learnable_params())\n        if USE_LINEAR_SCHEDULE:\n            self.scheduler = get_linear_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=self.warm_up_steps,\n                num_training_steps=12800,  # not sure what to set\n            )\n        return optimizer\n\n    def compute_loss(self, label_t: th.Tensor, label_p: th.Tensor) -> th.Tensor:\n        if self.loss_func == \"f1\":\n            label_loss = f1_loss(label_t, th.nn.functional.softmax(label_p, dim=1))\n        elif self.loss_func == \"ce+f1\":\n            label_loss = f1_loss(\n                label_t, th.nn.functional.softmax(label_p, dim=1)\n            ) + nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)(label_p, label_t)\n        else:\n            label_loss = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)(label_p, label_t)\n\n        return label_loss\n\n    def training_step(self, train_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            train_batch[\"img\"],\n            train_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.train_labels_t.append(label_t.detach().cpu())\n        self.train_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"train_loss\", label_loss, prog_bar=True)\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n\n        return label_loss\n\n    def on_train_epoch_end(self) -> None:\n        label_p = th.concatenate(self.train_labels_p)\n        label_t = th.concatenate(self.train_labels_t)\n\n        self.log_dict(\n            {\n                \"train_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.train_labels_t = []\n        self.train_labels_p = []\n\n    def validation_step(self, val_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            val_batch[\"img\"],\n            val_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.val_labels_t.append(label_t.detach().cpu())\n        self.val_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"val_loss\", label_loss, prog_bar=True)\n\n        return label_loss\n\n    def on_validation_epoch_end(self):\n        label_p = th.concatenate(self.val_labels_p)\n        label_t = th.concatenate(self.val_labels_t)\n\n        self.log_dict(\n            {\n                \"val_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.val_labels_t = []\n        self.val_labels_p = []\n\n    def on_epoch_end(self):\n        opt = self.optimizers(use_pl_optimizer=True)\n        self.log(\"lr\", opt.param_groups[0][\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.135802Z","iopub.execute_input":"2023-09-28T09:18:25.136504Z","iopub.status.idle":"2023-09-28T09:18:25.167558Z","shell.execute_reply.started":"2023-09-28T09:18:25.136470Z","shell.execute_reply":"2023-09-28T09:18:25.166582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiments","metadata":{}},{"cell_type":"code","source":"def _default_callbacks() -> List[Callback]:\n    return [\n        ModelCheckpoint(\n            monitor=\"val_f1_score\",\n            mode=\"max\",\n            save_top_k=2,\n            save_last=False,\n            filename=\"{epoch}-{val_loss}-{val_f1_score}-{val_multiclass_accuracy}\",\n        ),\n        \n    ]\n\n\nCLASS_DICT = {\n    \"albopictus\": th.tensor([1, 0, 0, 0, 0, 0], dtype=th.float),\n    \"culex\": th.tensor([0, 1, 0, 0, 0, 0], dtype=th.float),\n    \"japonicus/koreicus\": th.tensor([0, 0, 1, 0, 0, 0], dtype=th.float),\n    \"culiseta\": th.tensor([0, 0, 0, 1, 0, 0], dtype=th.float),\n    \"anopheles\": th.tensor([0, 0, 0, 0, 1, 0], dtype=th.float),\n    \"aegypti\": th.tensor([0, 0, 0, 0, 0, 1], dtype=th.float),\n}\n\n\nclass ExperimentMosquitoClassifier:\n    def __init__(\n        self,\n        img_dir: str,\n        annotations_csv: str,\n        class_dict: Dict[str, th.Tensor] = CLASS_DICT,\n    ):\n        self.img_dir = img_dir\n        self.annotations_csv = annotations_csv\n        self.class_dict = class_dict\n\n    def get_dataloaders(\n        self,\n        train_df: pd.DataFrame,\n        val_df: pd.DataFrame,\n        model_name: str,\n        data_aug: str,\n        bs: int,\n    ) -> List[DataLoader]:\n        transform = pre_process(model_name)\n\n        train_dataset = SimpleClassificationDataset(\n            train_df,\n            self.img_dir,\n            self.class_dict,\n            transform,\n            aug(data_aug),\n        )\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=bs,\n            shuffle=True,\n            num_workers=2,\n            drop_last=True,\n        )\n\n        val_dataset = SimpleClassificationDataset(\n            val_df,\n            self.img_dir,\n            self.class_dict,\n            transform,\n            aug(\"resize\"),\n            class_balance=False,\n        )\n        val_dataloader = DataLoader(\n            val_dataset,\n            batch_size=bs,\n            shuffle=False,\n            num_workers=2,\n        )\n\n        return train_dataloader, val_dataloader\n\n    def run(\n        self,\n        model_name: str,\n        dataset: str,\n        bs: int,\n        head_version: int,\n        data_aug: str,\n        freeze_backbones: bool = False,\n        warm_up_steps: int = 2000,\n        epochs: int = 5,\n        create_callbacks: Callable[[], List[Callback]] = _default_callbacks,\n    ):\n        annotations_df = pd.read_csv(self.annotations_csv)\n        train_df, val_df = train_test_split(\n            annotations_df,\n            test_size=0.2,\n            stratify=annotations_df[\"class_label\"],\n            random_state=200,\n        )\n\n        train_dataloader, val_dataloader = self.get_dataloaders(\n            train_df, val_df, model_name, data_aug, bs\n        )\n\n        th.set_float32_matmul_precision(\"high\")\n        model = MosquitoClassifier(\n            model_name=model_name,\n            dataset=dataset,\n            freeze_backbones=freeze_backbones,\n            head_version=head_version,\n            warm_up_steps=warm_up_steps,\n            bs=bs,\n            data_aug=data_aug,\n            epochs=epochs,\n        )\n        trainer = pl.Trainer(\n            accelerator=\"gpu\",\n            precision=\"16-mixed\",\n            max_epochs=epochs,\n            logger=True,\n            deterministic=True,  # maybe we should add this\n            callbacks=create_callbacks(),\n        )\n\n        trainer.fit(\n            model=model,\n            train_dataloaders=train_dataloader,\n            val_dataloaders=val_dataloader,\n        )\n\n    def run_cross_validation(\n        self,\n        model_name: str,\n        dataset: str,\n        bs: int,\n        head_version: int,\n        data_aug: str,\n        freeze_backbones: bool = False,\n        warm_up_steps: int = 2000,\n        epochs: int = 5,\n        n_splits: int = 5,\n        create_callbacks: Callable[[], List[Callback]] = _default_callbacks,\n    ):\n        annotations_df = pd.read_csv(self.annotations_csv)\n        skf = StratifiedKFold(n_splits=n_splits)\n\n        for _, (train_index, val_index) in enumerate(\n            skf.split(annotations_df, annotations_df.class_label)\n        ):\n            train_df = annotations_df.iloc[train_index]\n            val_df = annotations_df.iloc[val_index]\n\n            train_dataloader, val_dataloader = self.get_dataloaders(\n                train_df, val_df, model_name, data_aug, bs\n            )\n\n            th.set_float32_matmul_precision(\"high\")\n            model = MosquitoClassifier(\n                model_name=model_name,\n                dataset=dataset,\n                freeze_backbones=freeze_backbones,\n                head_version=head_version,\n                warm_up_steps=warm_up_steps,\n                bs=bs,\n                data_aug=data_aug,\n                epochs=epochs,\n            )\n            trainer = pl.Trainer(\n                accelerator=\"gpu\",\n                precision=\"16-mixed\",\n                max_epochs=epochs,\n                logger=True,\n                callbacks=create_callbacks(),  # if I pass it as list of callbacks it doesn't work\n                deterministic=True,  # maybe we should add this\n                # TODO: we need some naming convention\n            )\n\n            trainer.fit(\n                model=model,\n                train_dataloaders=train_dataloader,\n                val_dataloaders=val_dataloader,\n            )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.168814Z","iopub.execute_input":"2023-09-28T09:18:25.169807Z","iopub.status.idle":"2023-09-28T09:18:25.193553Z","shell.execute_reply.started":"2023-09-28T09:18:25.169773Z","shell.execute_reply":"2023-09-28T09:18:25.192654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.194848Z","iopub.execute_input":"2023-09-28T09:18:25.195405Z","iopub.status.idle":"2023-09-28T09:18:25.207423Z","shell.execute_reply.started":"2023-09-28T09:18:25.195366Z","shell.execute_reply":"2023-09-28T09:18:25.206309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df = pd.read_csv(annotations_csv)\nannotations_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.210748Z","iopub.execute_input":"2023-09-28T09:18:25.211285Z","iopub.status.idle":"2023-09-28T09:18:25.267897Z","shell.execute_reply.started":"2023-09-28T09:18:25.211242Z","shell.execute_reply":"2023-09-28T09:18:25.266990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n\nclass_dict = {\n    \"albopictus\": th.tensor([1, 0, 0, 0, 0, 0], dtype=th.long),\n    \"culex\": th.tensor([0, 1, 0, 0, 0, 0], dtype=th.long),\n    \"japonicus/koreicus\": th.tensor([0, 0, 1, 0, 0, 0], dtype=th.long),\n    \"culiseta\": th.tensor([0, 0, 0, 1, 0, 0], dtype=th.long),\n    \"anopheles\": th.tensor([0, 0, 0, 0, 1, 0], dtype=th.long),\n    \"aegypti\": th.tensor([0, 0, 0, 0, 0, 1], dtype=th.long),\n}\n\ntransform = pre_process(\"\")\n\ndata_augmentation = aug(\"image_net\")\n\nds = SimpleClassificationDataset(\n    annotations_df=annotations_df,\n    img_dir=img_dir,\n    class_dict=class_dict,\n    transform=transform,\n    data_augment=data_augmentation,\n)\nfor i in range(10):\n    res = ds[i]\n    img = res[\"img\"]\n\n    img_bbox = th.tensor(\n        255 * (img - img.min()) / (img.max() - img.min()), dtype=th.uint8\n    )\n    show(img_bbox)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:25.269412Z","iopub.execute_input":"2023-09-28T09:18:25.269793Z","iopub.status.idle":"2023-09-28T09:18:30.377173Z","shell.execute_reply.started":"2023-09-28T09:18:25.269757Z","shell.execute_reply":"2023-09-28T09:18:30.376274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = ExperimentMosquitoClassifier(img_dir, annotations_csv)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:30.378430Z","iopub.execute_input":"2023-09-28T09:18:30.380216Z","iopub.status.idle":"2023-09-28T09:18:30.385008Z","shell.execute_reply.started":"2023-09-28T09:18:30.380181Z","shell.execute_reply":"2023-09-28T09:18:30.383842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp.run(\n    model_name = MODEL_NAME,\n    dataset = PRETRAIN_DATASET,\n    bs = BATCH_SIZE,\n    head_version = HEAD_NUMBER,\n    data_aug = AUGMENTATION,\n    freeze_backbones = FREEZE_BACKBONE,\n    warm_up_steps = WARMUP_STEPS,\n    epochs = EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:18:30.386684Z","iopub.execute_input":"2023-09-28T09:18:30.387431Z","iopub.status.idle":"2023-09-28T16:04:52.986475Z","shell.execute_reply.started":"2023-09-28T09:18:30.387388Z","shell.execute_reply":"2023-09-28T16:04:52.984861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bounding Boxes","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nfrom pathlib import Path\nimport cv2\nfrom timeit import default_timer as timer\nimport timeit\nimport time\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection","metadata":{"execution":{"iopub.status.busy":"2023-09-28T08:39:37.263638Z","iopub.execute_input":"2023-09-28T08:39:37.264382Z","iopub.status.idle":"2023-09-28T08:39:52.328165Z","shell.execute_reply.started":"2023-09-28T08:39:37.264329Z","shell.execute_reply":"2023-09-28T08:39:52.327070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/')\nowl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/').cpu()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T08:39:52.330561Z","iopub.execute_input":"2023-09-28T08:39:52.331028Z","iopub.status.idle":"2023-09-28T08:39:59.995956Z","shell.execute_reply.started":"2023-09-28T08:39:52.330985Z","shell.execute_reply":"2023-09-28T08:39:59.994897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[\"a photo of a mosquito\"]]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T08:39:59.997153Z","iopub.execute_input":"2023-09-28T08:39:59.997474Z","iopub.status.idle":"2023-09-28T08:40:00.523674Z","shell.execute_reply.started":"2023-09-28T08:39:59.997446Z","shell.execute_reply":"2023-09-28T08:40:00.522508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = time.time()\nwith torch.no_grad():\n    inputs = owl_processor(text=texts, images=image, padding='do_not_pad', return_tensors=\"pt\")\n    outputs = owl_model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n\n    # Convert outputs (bounding boxes and class logits) to COCO API\n    results = owl_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.01)\ne = time.time()\nprint(\"OWL Time \", 1000 * (e - s), \"ms\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T08:40:34.216933Z","iopub.execute_input":"2023-09-28T08:40:34.217389Z","iopub.status.idle":"2023-09-28T08:40:35.562263Z","shell.execute_reply.started":"2023-09-28T08:40:34.217356Z","shell.execute_reply":"2023-09-28T08:40:35.561355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef locate_object():\n    # inputs = owl_processor(text=texts, images=image, padding='do_not_pad', return_tensors=\"pt\")\n    inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\")\n    outputs = owl_model(**inputs)\n    target_sizes = torch.Tensor([image.size[::-1]])\n\n    # Convert outputs (bounding boxes and class logits) to COCO API\n    results = owl_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.01)\n\ntimeit.timeit(locate_object, number=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    text_tokens = owl_processor(text=texts, return_tensors=\"pt\")\n\n@torch.no_grad()\ndef locate_object():\n    # inputs = owl_processor(text=texts, images=image, padding='do_not_pad', return_tensors=\"pt\")\n    # inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\")\n    outputs = owl_model(**text_tokens, **owl_processor.image_processor(images=image, return_tensors=\"pt\", size=768))\n    target_sizes = torch.Tensor([image.size[::-1]])\n\n    # Convert outputs (bounding boxes and class logits) to COCO API\n    results = owl_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.01)\n\ntimeit.timeit(locate_object, number=20)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T08:41:42.475770Z","iopub.execute_input":"2023-09-28T08:41:42.476254Z","iopub.status.idle":"2023-09-28T08:42:05.921225Z","shell.execute_reply.started":"2023-09-28T08:41:42.476219Z","shell.execute_reply":"2023-09-28T08:42:05.920020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import YolosImageProcessor, YolosForObjectDetection\nfrom PIL import Image\nimport torch\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n\n\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0.csv\"\nroot_images = os.path.join(img_dir)\n\nall_images = os.listdir(root_images)\nprint(f\"Total images: {len(all_images)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image.size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_image_file = os.path.join(root_images, all_images[3])\nimage = Image.open(original_image_file)\n\nwith torch.no_grad():\n    inputs = image_processor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n\n\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )\nresults['iou'] = np.array([bb_intersection_over_union(box.tolist(), [0, 0, *image.size]) for box in results[\"boxes\"]])\nprint(results['iou'])\nresults[\"scores\"] = results[\"scores\"][results['iou']<0.95]\nresults[\"boxes\"] = results[\"boxes\"][results['iou']<0.95]\n    \nbest_score_index = np.argmax(results[\"scores\"].numpy())\nbox = results[\"boxes\"][best_score_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure and axes\nfig, ax = plt.subplots()\n\n# Display the image\nax.imshow(image)\n\n# Create a Rectangle patch\nbox = [round(i, 2) for i in box.tolist()]\nrect_owl = patches.Rectangle(box[:2], box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none', label='Owl-ViT')\nax.add_patch(rect_owl)\nplt.legend()\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/')\nowl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/').cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_images = os.path.join(img_dir)\n\nall_images = os.listdir(root_images)\nprint(f\"Total images: {len(all_images)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = []\nfor original_image in tqdm(all_images):\n    original_image_file = os.path.join(root_images, original_image)\n    image = Image.open(original_image_file)\n    with torch.no_grad():\n        texts = [[\"a photo of a mosquito\"]]\n        inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\").to('cuda')\n        outputs = owl_model(**inputs)\n        # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n        target_sizes = torch.Tensor([image.size[::-1]]).to('cuda')\n\n        # Convert outputs (bounding boxes and class logits) to COCO API\n        results = owl_processor.post_process_object_detection(outputs=outputs, \n                                                            target_sizes=target_sizes, \n                                                            threshold=0.01)\n        \n        i = 0  # Retrieve predictions for the first image for the corresponding text queries\n        text = texts[i]\n        boxes, scores, labels = results[i][\"boxes\"].cpu().numpy(), results[i][\"scores\"].cpu().numpy(), results[i][\"labels\"].cpu().numpy()\n        best_score_index = np.argmax(scores)\n        boxes, scores, labels = boxes[best_score_index], scores[best_score_index], labels[best_score_index]\n        row = [original_image, boxes[0], boxes[1], boxes[2], boxes[3]]\n        rows.append(row)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(rows, columns=[\"img_fName\", \"bbx_xtl\", \"bbx_ytl\", \"bbx_xbr\", \"bbx_ybr\"])\ndf.to_csv('/kaggle/working/owl_vit_image_bboxes.csv', index=False)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df = pd.read_csv(annotations_csv)\nannotations_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged = annotations_df.merge(df, on='img_fName', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['bbox_true'] = df_merged.apply(lambda x: [x.bbx_xtl_x, x.bbx_ytl_x, x.bbx_xbr_x, x.bbx_ybr_x], axis=1)\ndf_merged['bbox_owl'] = df_merged.apply(lambda x: [x.bbx_xtl_y, x.bbx_ytl_y, x.bbx_xbr_y, x.bbx_ybr_y], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['IoU'] = df_merged.apply(lambda x: bb_intersection_over_union(x.bbox_true, x.bbox_owl), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['IoU'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged[df_merged['IoU']<0.1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 10224\n\nim = Image.open(img_dir+df_merged['img_fName'].iloc[idx])\n\n# Create figure and axes\nfig, ax = plt.subplots()\n\n# Display the image\nax.imshow(im)\n\n# Create a Rectangle patch\nrect_baseline = patches.Rectangle(\n    df_merged['bbox_true'].iloc[idx][:2], \n    df_merged['bbox_true'].iloc[idx][2]-df_merged['bbox_true'].iloc[idx][0],\n    df_merged['bbox_true'].iloc[idx][3]-df_merged['bbox_true'].iloc[idx][1], linewidth=1, edgecolor='b', facecolor='none', label='ground truth')\n# Add the patch to the Axes\nax.add_patch(rect_baseline)\n\n# Create a Rectangle patch\nrect_owl = patches.Rectangle(\n    df_merged['bbox_owl'].iloc[idx][:2], \n    df_merged['bbox_owl'].iloc[idx][2]-df_merged['bbox_owl'].iloc[idx][0],\n    df_merged['bbox_owl'].iloc[idx][3]-df_merged['bbox_owl'].iloc[idx][1], linewidth=1, edgecolor='r', facecolor='none', label='Owl-ViT')\n# Add the patch to the Axes\nax.add_patch(rect_owl)\nplt.legend()\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}