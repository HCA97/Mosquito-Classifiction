{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torcheval","metadata":{"execution":{"iopub.status.busy":"2023-09-13T16:52:06.198473Z","iopub.execute_input":"2023-09-13T16:52:06.199169Z","iopub.status.idle":"2023-09-13T16:52:22.479771Z","shell.execute_reply.started":"2023-09-13T16:52:06.199135Z","shell.execute_reply":"2023-09-13T16:52:22.478410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:42:22.352721Z","iopub.execute_input":"2023-09-09T17:42:22.353166Z","iopub.status.idle":"2023-09-09T17:42:34.855229Z","shell.execute_reply.started":"2023-09-09T17:42:22.353126Z","shell.execute_reply":"2023-09-09T17:42:34.853895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict, Callable, Optional, Union, Tuple, Any\n\nimport os\nimport logging\n\nimport random\n\nimport torch as th\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, Callback\nfrom pytorch_lightning.utilities.types import STEP_OUTPUT\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy\nfrom transformers import get_linear_schedule_with_warmup\n\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as T\nimport albumentations as A\n\nimport open_clip","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:42:34.858008Z","iopub.execute_input":"2023-09-09T17:42:34.858371Z","iopub.status.idle":"2023-09-09T17:42:50.241319Z","shell.execute_reply.started":"2023-09-09T17:42:34.858334Z","shell.execute_reply":"2023-09-09T17:42:50.240298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class HeadV1(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(),\n            nn.LeakyReLU(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV4(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV5(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(p=0.9),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV6(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Dropout1d(p=0.75),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV2(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass HeadV3(nn.Module):\n    def __init__(self, f_out: int, f_in: int):\n        super().__init__()\n\n        self.label = nn.Sequential(\n            nn.BatchNorm1d(f_in),\n            nn.Linear(f_in, f_in, bias=False),\n            nn.BatchNorm1d(f_in),\n            nn.LeakyReLU(),\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_in, bias=False),\n            nn.BatchNorm1d(f_in),\n            nn.LeakyReLU(),\n            nn.Dropout1d(),\n            nn.Linear(f_in, f_out),\n        )\n\n    def forward(self, x):\n        return self.label(x)\n\n\nclass CLIPClassifier(nn.Module):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        data: str = \"datacomp_xl_s13b_b90k\",\n        head_version: int = 1,\n    ):\n        super().__init__()\n        self.backbone = open_clip.create_model_and_transforms(\n            model_name, pretrained=data\n        )[0].visual\n\n        if model_name == \"ViT-L-14\":\n            self.n = 768\n            self.lrs = dict(\n                back_lrs={\"8\": 1.25e-6, \"16\": 2.5e-6, \"20\": 5e-6, \"24\": 10e-6},\n                back_wd=1e-3,\n                hd_lr=3e-4,\n                hd_wd=1e-5,\n            )\n        elif model_name == \"ViT-H-14\":\n            self.n = 1024\n            self.lrs = {\n                \"back_lrs\": {\"10\": 1.25e-6, \"20\": 2.5e-6, \"26\": 5e-6, \"32\": 10e-6},\n                \"back_wd\": 1e-3,\n                \"hd_lr\": 3e-4,\n                \"hd_wd\": 1e-5,\n            }\n        elif model_name == \"ViT-B-16\":\n            self.n = 512\n            self.lrs = {\n                \"back_lrs\": {\"1\": 2.5e-6, \"7\": 5e-6, \"12\": 10e-6},\n                \"back_wd\": 1e-3,\n                \"hd_lr\": 3e-4,\n                \"hd_wd\": 1e-5,\n            }\n        else:\n            raise ValueError\n\n        if head_version == 2:\n            self.label = HeadV2(n_classes, self.n)\n        elif head_version == 3:\n            self.label = HeadV3(n_classes, self.n)\n        elif head_version == 4:\n            self.label = HeadV4(n_classes, self.n)\n        elif head_version == 5:\n            self.label = HeadV5(n_classes, self.n)\n        elif head_version == 6:\n            self.label = HeadV6(n_classes, self.n)\n        else:\n            self.label = HeadV1(n_classes, self.n)\n\n        self.n_classes = n_classes\n\n    def forward(self, x: th.tensor) -> th.tensor:\n        x = self.backbone(x)\n        return self.label(x)\n\n    def get_parameter_section(self, parameters, lr=None, wd=None):\n        # https://github.com/IvanAer/G-Universal-CLIP\n        parameter_settings = []\n\n        lr_is_dict = isinstance(lr, dict)\n        wd_is_dict = isinstance(wd, dict)\n\n        layer_no = None\n        for n, p in parameters:\n            for split in n.split(\".\"):\n                if split.isnumeric():\n                    layer_no = int(split)\n\n            if not layer_no:\n                layer_no = 0\n\n            if lr_is_dict:\n                for k, v in lr.items():\n                    if layer_no < int(k):\n                        temp_lr = v\n                        break\n            else:\n                temp_lr = lr\n\n            if wd_is_dict:\n                for k, v in wd.items():\n                    if layer_no < int(k):\n                        temp_wd = v\n                        break\n            else:\n                temp_wd = wd\n\n            parameter_setting = {\"params\": p, \"lr\": temp_lr, \"weight_decay\": temp_wd}\n            parameter_settings.append(parameter_setting)\n        return parameter_settings\n\n    def get_learnable_params(self) -> list:\n        back_lrs = self.lrs[\"back_lrs\"]\n        back_wd = self.lrs[\"back_wd\"]\n        hd_lr = self.lrs[\"hd_lr\"]\n        hd_wd = self.lrs[\"hd_wd\"]\n\n        parameter_settings = []\n\n        if back_lrs and back_wd:\n            parameter_settings.extend(\n                self.get_parameter_section(\n                    [(n, p) for n, p in self.backbone.named_parameters()],\n                    lr=back_lrs,\n                    wd=back_wd,\n                )\n            )\n\n        parameter_settings.extend(\n            self.get_parameter_section(\n                [(n, p) for n, p in self.label.named_parameters()], lr=hd_lr, wd=hd_wd\n            )\n        )\n\n        return parameter_settings","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:42:50.243820Z","iopub.execute_input":"2023-09-09T17:42:50.244171Z","iopub.status.idle":"2023-09-09T17:42:50.280468Z","shell.execute_reply.started":"2023-09-09T17:42:50.244137Z","shell.execute_reply":"2023-09-09T17:42:50.279389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"def pre_process(_: str) -> T.Compose:\n    return T.Compose(\n        [\n            T.ToTensor(),\n            T.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073),\n                std=(0.26862954, 0.26130258, 0.27577711),\n            ),\n        ]\n    )\n\n\ndef aug(data_aug: str = \"image_net\") -> T.Compose:\n    transform = T.Compose(\n        [\n            T.ToPILImage(),\n            T.Resize(\n                size=(224, 224),\n                interpolation=T.InterpolationMode.BICUBIC,\n                antialias=True,\n            ),\n        ]\n    )\n    if data_aug == \"image_net\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n\n    elif data_aug == \"hca\":\n        aug8p3 = A.OneOf(\n            [\n                A.Sharpen(p=0.3),\n                A.ToGray(p=0.3),\n                A.CLAHE(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        blur = A.OneOf(\n            [\n                A.GaussianBlur(p=0.3),\n                A.MotionBlur(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        transform = A.Compose(\n            [\n                A.ShiftScaleRotate(\n                    rotate_limit=45,\n                    scale_limit=0.1,\n                    border_mode=cv2.BORDER_REFLECT,\n                    interpolation=cv2.INTER_CUBIC,\n                    p=0.5,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                aug8p3,\n                blur,\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.ElasticTransform(p=0.5),\n                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            ]\n        )\n    elif data_aug == \"aug_mix\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AugMix(),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n    elif data_aug == \"happy_whale\":\n        aug8p3 = A.OneOf(\n            [\n                A.Sharpen(p=0.3),\n                A.ToGray(p=0.3),\n                A.CLAHE(p=0.3),\n            ],\n            p=0.5,\n        )\n\n        transform = A.Compose(\n            [\n                A.ShiftScaleRotate(\n                    rotate_limit=15,\n                    scale_limit=0.1,\n                    border_mode=cv2.BORDER_REFLECT,\n                    p=0.5,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                aug8p3,\n                A.HorizontalFlip(p=0.5),\n                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            ]\n        )\n\n    elif data_aug == \"cut_out\":\n        transform = A.Compose(\n            [\n                A.HorizontalFlip(p=0.5),\n                A.ImageCompression(quality_lower=99, quality_upper=100),\n                A.ShiftScaleRotate(\n                    shift_limit=0.2,\n                    scale_limit=0.2,\n                    rotate_limit=10,\n                    border_mode=cv2.BORDER_REFLECT,\n                    p=0.7,\n                ),\n                A.Resize(224, 224, cv2.INTER_CUBIC),\n                A.Cutout(\n                    max_h_size=int(224 * 0.4),\n                    max_w_size=int(224 * 0.4),\n                    num_holes=1,\n                    p=0.5,\n                ),\n            ]\n        )\n    elif data_aug == \"clip\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.RandomResizedCrop(\n                    size=(224, 224),\n                    scale=(0.9, 1.0),\n                    ratio=(0.75, 1.3333),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n    elif data_aug == \"clip+image_net\":\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n                T.RandomResizedCrop(\n                    size=(224, 224),\n                    scale=(0.9, 1.0),\n                    ratio=(0.75, 1.3333),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n                T.Resize(\n                    size=(224, 224),\n                    interpolation=T.InterpolationMode.BICUBIC,\n                    antialias=True,\n                ),\n            ]\n        )\n\n    return transform\n\n\ndef read_image_cv2(f_name: str, gray_scale: bool = False) -> np.ndarray:\n    img = cv2.imread(\n        f_name, cv2.IMREAD_ANYCOLOR if not gray_scale else cv2.IMREAD_GRAYSCALE\n    )\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\n\ndef class_balancing(df: pd.DataFrame) -> pd.DataFrame:\n    counts = df.class_label.value_counts().to_dict()\n    max_label = max(list(counts.items()), key=lambda x: x[1])\n\n    for key, value in counts.items():\n        if key == max_label[0]:\n            continue\n\n        df_label = df[df.class_label == key].sample(\n            n=max_label[1] - value, replace=True\n        )\n        df = pd.concat([df, df_label])\n\n    return df\n\n\nclass SimpleClassificationDataset(Dataset):\n    def __init__(\n        self,\n        annotations_df: pd.DataFrame,\n        img_dir: str,\n        class_dict: dict,\n        transform: Optional[T.Compose] = None,\n        data_augment: Optional[Union[T.Compose, A.Compose]] = None,\n        class_balance: bool = True,\n    ):\n        self.df = annotations_df\n        if class_balance:\n            self.df = class_balancing(annotations_df)\n\n        self.img_dir = img_dir\n        self.class_dict = class_dict\n        self.transform = transform\n        self.data_augment = data_augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        cv2.setNumThreads(6)\n\n        f_name, _, _, x_tl, y_tl, x_br, y_br, label = self.df.iloc[idx]\n\n        img = read_image_cv2(os.path.join(self.img_dir, f_name))\n        img_ = img[y_tl:y_br, x_tl:x_br, :]\n        if img_.shape[0] * img_.shape[1] != 0:\n            img = img_\n\n        if self.data_augment:\n            if isinstance(self.data_augment, A.Compose):\n                img = self.data_augment(image=img)[\"image\"]\n            else:\n                img = self.data_augment(img)\n\n        if self.transform:\n            img = self.transform(img)\n\n        if self.class_dict:\n            label = self.class_dict[label]\n        return {\"img\": img, \"label\": label}","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:42:50.282182Z","iopub.execute_input":"2023-09-09T17:42:50.282559Z","iopub.status.idle":"2023-09-09T17:42:50.314677Z","shell.execute_reply.started":"2023-09-09T17:42:50.282524Z","shell.execute_reply":"2023-09-09T17:42:50.313656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"def f1(y_true: th.Tensor, y_pred: th.Tensor):\n    y_pred = th.round(y_pred)\n    tp = th.sum((y_true * y_pred).float(), dim=0)\n    tn = th.sum(((1 - y_true) * (1 - y_pred)).float(), dim=0)\n    fp = th.sum(((1 - y_true) * y_pred).float(), dim=0)\n    fn = th.sum((y_true * (1 - y_pred)).float(), dim=0)\n\n    p = tp / (tp + fp + 1e-7)\n    r = tp / (tp + fn + 1e-7)\n\n    f1 = 2 * p * r / (p + r + 1e-7)\n    f1 = th.where(th.isnan(f1), th.zeros_like(f1), f1)\n    return th.mean(f1)\n\n\ndef f1_loss(y_true: th.Tensor, y_pred: th.Tensor):\n    tp = th.sum((y_true * y_pred).float(), dim=0)\n    tn = th.sum(((1 - y_true) * (1 - y_pred)).float(), dim=0)\n    fp = th.sum(((1 - y_true) * y_pred).float(), dim=0)\n    fn = th.sum((y_true * (1 - y_pred)).float(), dim=0)\n\n    p = tp / (tp + fp + 1e-7)\n    r = tp / (tp + fn + 1e-7)\n\n    f1 = 2 * p * r / (p + r + 1e-7)\n    f1 = th.where(th.isnan(f1), th.zeros_like(f1), f1)\n    return 1 - th.mean(f1)\n\n\ndef accuracy(y1: th.Tensor, y2: th.Tensor):\n    y1_argmax = y1.argmax(dim=1)\n    y2_argmax = y2.argmax(dim=1)\n\n    correct_sum = th.sum(y1_argmax == y2_argmax)\n    return correct_sum / len(y1)\n\n\nclass MosquitoClassifier(pl.LightningModule):\n    def __init__(\n        self,\n        n_classes: int = 6,\n        model_name: str = \"ViT-L-14\",\n        dataset: str = \"datacomp_xl_s13b_b90k\",\n        freeze_backbones: bool = False,\n        head_version: int = 0,\n        warm_up_steps: int = 2000,\n        bs: int = 64,\n        data_aug: str = \"\",\n        loss_func: str = \"ce\",\n        epochs: int = 5,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.cls = CLIPClassifier(n_classes, model_name, dataset, head_version)\n        if freeze_backbones:\n            self.freezebackbone()\n\n        self.scheduler = None\n        self.n_classes = n_classes\n        self.warm_up_steps = warm_up_steps\n        self.loss_func = loss_func\n\n        self.val_labels_t = []\n        self.val_labels_p = []\n\n        self.train_labels_t = []\n        self.train_labels_p = []\n\n    def freezebackbone(self) -> None:\n        for param in self.cls.backbone.parameters():\n            param.requires_grad = False\n\n    def forward(self, x: th.Tensor) -> th.Tensor:\n        return self.cls(x)\n\n    def lr_schedulers(self):\n        # over-write this shit\n        return self.scheduler\n\n    def configure_optimizers(self):\n        optimizer = th.optim.AdamW(self.cls.get_learnable_params())\n        self.scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=self.warm_up_steps,\n            num_training_steps=12800,  # not sure what to set\n        )\n        return optimizer\n\n    def compute_loss(self, label_t: th.Tensor, label_p: th.Tensor) -> th.Tensor:\n        if self.loss_func == \"f1\":\n            label_loss = f1_loss(label_t, th.nn.functional.softmax(label_p, dim=1))\n        elif self.loss_func == \"ce+f1\":\n            label_loss = f1_loss(\n                label_t, th.nn.functional.softmax(label_p, dim=1)\n            ) + nn.CrossEntropyLoss()(label_p, label_t)\n        else:\n            label_loss = nn.CrossEntropyLoss()(label_p, label_t)\n\n        return label_loss\n\n    def training_step(self, train_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            train_batch[\"img\"],\n            train_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.train_labels_t.append(label_t.detach().cpu())\n        self.train_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"train_loss\", label_loss)\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n\n        return label_loss\n\n    def on_train_epoch_end(self) -> None:\n        label_p = th.concatenate(self.train_labels_p)\n        label_t = th.concatenate(self.train_labels_t)\n\n        self.log_dict(\n            {\n                \"train_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"train_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.train_labels_t = []\n        self.train_labels_p = []\n\n    def validation_step(self, val_batch, batch_idx) -> STEP_OUTPUT:\n        img, label_t = (\n            val_batch[\"img\"],\n            val_batch[\"label\"],\n        )\n\n        label_p = self.cls(img)\n        label_loss = self.compute_loss(label_t, label_p)\n\n        self.val_labels_t.append(label_t.detach().cpu())\n        self.val_labels_p.append(label_p.detach().cpu())\n\n        self.log(\"val_loss\", label_loss)\n\n        return label_loss\n\n    def on_validation_epoch_end(self):\n        label_p = th.concatenate(self.val_labels_p)\n        label_t = th.concatenate(self.val_labels_t)\n\n        self.log_dict(\n            {\n                \"val_f1_score\": multiclass_f1_score(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_multiclass_accuracy\": multiclass_accuracy(\n                    label_p,\n                    label_t.argmax(dim=1),\n                    num_classes=self.n_classes,\n                    average=\"macro\",\n                ),\n                \"val_accuracy\": accuracy(label_t, label_p),\n            }\n        )\n\n        self.val_labels_t = []\n        self.val_labels_p = []\n\n    def on_epoch_end(self):\n        opt = self.optimizers(use_pl_optimizer=True)\n        self.log(\"lr\", opt.param_groups[0][\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:42:53.283991Z","iopub.execute_input":"2023-09-09T17:42:53.284349Z","iopub.status.idle":"2023-09-09T17:42:53.315410Z","shell.execute_reply.started":"2023-09-09T17:42:53.284320Z","shell.execute_reply":"2023-09-09T17:42:53.314347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiments","metadata":{}},{"cell_type":"code","source":"def _default_callbacks() -> List[Callback]:\n    return [\n        ModelCheckpoint(\n            monitor=\"val_f1_score\",\n            mode=\"max\",\n            save_top_k=2,\n            save_last=True,\n            filename=\"{epoch}-{val_loss}-{val_f1_score}-{val_multiclass_accuracy}\",\n        ),\n    ]\n\n\nCLASS_DICT = {\n    \"albopictus\": th.tensor([1, 0, 0, 0, 0, 0], dtype=th.float),\n    \"culex\": th.tensor([0, 1, 0, 0, 0, 0], dtype=th.float),\n    \"japonicus/koreicus\": th.tensor([0, 0, 1, 0, 0, 0], dtype=th.float),\n    \"culiseta\": th.tensor([0, 0, 0, 1, 0, 0], dtype=th.float),\n    \"anopheles\": th.tensor([0, 0, 0, 0, 1, 0], dtype=th.float),\n    \"aegypti\": th.tensor([0, 0, 0, 0, 0, 1], dtype=th.float),\n}\n\n\nclass ExperimentMosquitoClassifier:\n    def __init__(\n        self,\n        img_dir: str,\n        annotations_csv: str,\n        class_dict: Dict[str, th.Tensor] = CLASS_DICT,\n    ):\n        self.img_dir = img_dir\n        self.annotations_csv = annotations_csv\n        self.class_dict = class_dict\n\n    def get_dataloaders(\n        self,\n        train_df: pd.DataFrame,\n        val_df: pd.DataFrame,\n        model_name: str,\n        data_aug: str,\n        bs: int,\n    ) -> List[DataLoader]:\n        transform = pre_process(model_name)\n\n        train_dataset = SimpleClassificationDataset(\n            train_df,\n            self.img_dir,\n            self.class_dict,\n            transform,\n            aug(data_aug),\n        )\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=bs,\n            shuffle=True,\n            num_workers=2,\n            drop_last=True,\n        )\n\n        val_dataset = SimpleClassificationDataset(\n            val_df,\n            self.img_dir,\n            self.class_dict,\n            transform,\n            aug(\"resize\"),\n            class_balance=False,\n        )\n        val_dataloader = DataLoader(\n            val_dataset,\n            batch_size=bs,\n            shuffle=False,\n            num_workers=2,\n        )\n\n        return train_dataloader, val_dataloader\n\n    def run(\n        self,\n        model_name: str,\n        dataset: str,\n        bs: int,\n        head_version: int,\n        data_aug: str,\n        freeze_backbones: bool = False,\n        warm_up_steps: int = 2000,\n        epochs: int = 5,\n        create_callbacks: Callable[[], List[Callback]] = _default_callbacks,\n    ):\n        annotations_df = pd.read_csv(self.annotations_csv)\n        train_df, val_df = train_test_split(\n            annotations_df,\n            test_size=0.2,\n            stratify=annotations_df[\"class_label\"],\n            random_state=200,\n        )\n\n        train_dataloader, val_dataloader = self.get_dataloaders(\n            train_df, val_df, model_name, data_aug, bs\n        )\n\n        th.set_float32_matmul_precision(\"high\")\n        model = MosquitoClassifier(\n            model_name=model_name,\n            dataset=dataset,\n            freeze_backbones=freeze_backbones,\n            head_version=head_version,\n            warm_up_steps=warm_up_steps,\n            bs=bs,\n            data_aug=data_aug,\n            epochs=epochs,\n        )\n        trainer = pl.Trainer(\n            accelerator=\"gpu\",\n            precision=\"16-mixed\",\n            max_epochs=epochs,\n            logger=True,\n            deterministic=True,  # maybe we should add this\n            callbacks=create_callbacks(),\n        )\n\n        trainer.fit(\n            model=model,\n            train_dataloaders=train_dataloader,\n            val_dataloaders=val_dataloader,\n        )\n\n    def run_cross_validation(\n        self,\n        model_name: str,\n        dataset: str,\n        bs: int,\n        head_version: int,\n        data_aug: str,\n        freeze_backbones: bool = False,\n        warm_up_steps: int = 2000,\n        epochs: int = 5,\n        n_splits: int = 5,\n        create_callbacks: Callable[[], List[Callback]] = _default_callbacks,\n    ):\n        annotations_df = pd.read_csv(self.annotations_csv)\n        skf = StratifiedKFold(n_splits=n_splits)\n\n        for _, (train_index, val_index) in enumerate(\n            skf.split(annotations_df, annotations_df.class_label)\n        ):\n            train_df = annotations_df.iloc[train_index]\n            val_df = annotations_df.iloc[val_index]\n\n            train_dataloader, val_dataloader = self.get_dataloaders(\n                train_df, val_df, model_name, data_aug, bs\n            )\n\n            th.set_float32_matmul_precision(\"high\")\n            model = MosquitoClassifier(\n                model_name=model_name,\n                dataset=dataset,\n                freeze_backbones=freeze_backbones,\n                head_version=head_version,\n                warm_up_steps=warm_up_steps,\n                bs=bs,\n                data_aug=data_aug,\n                epochs=epochs,\n            )\n            trainer = pl.Trainer(\n                accelerator=\"gpu\",\n                precision=\"16-mixed\",\n                max_epochs=epochs,\n                logger=True,\n                callbacks=create_callbacks(),  # if I pass it as list of callbacks it doesn't work\n                deterministic=True,  # maybe we should add this\n                # TODO: we need some naming convention\n            )\n\n            trainer.fit(\n                model=model,\n                train_dataloaders=train_dataloader,\n                val_dataloaders=val_dataloader,\n            )","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:42:56.998439Z","iopub.execute_input":"2023-09-09T17:42:56.998822Z","iopub.status.idle":"2023-09-09T17:42:57.022577Z","shell.execute_reply.started":"2023-09-09T17:42:56.998784Z","shell.execute_reply":"2023-09-09T17:42:57.021616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:43:00.100935Z","iopub.execute_input":"2023-09-09T17:43:00.101302Z","iopub.status.idle":"2023-09-09T17:43:00.106278Z","shell.execute_reply.started":"2023-09-09T17:43:00.101272Z","shell.execute_reply":"2023-09-09T17:43:00.105230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df = pd.read_csv(annotations_csv)\nannotations_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:43:01.959947Z","iopub.execute_input":"2023-09-09T17:43:01.960311Z","iopub.status.idle":"2023-09-09T17:43:02.028721Z","shell.execute_reply.started":"2023-09-09T17:43:01.960281Z","shell.execute_reply":"2023-09-09T17:43:02.024048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n\nclass_dict = {\n    \"albopictus\": th.tensor([1, 0, 0, 0, 0, 0], dtype=th.long),\n    \"culex\": th.tensor([0, 1, 0, 0, 0, 0], dtype=th.long),\n    \"japonicus/koreicus\": th.tensor([0, 0, 1, 0, 0, 0], dtype=th.long),\n    \"culiseta\": th.tensor([0, 0, 0, 1, 0, 0], dtype=th.long),\n    \"anopheles\": th.tensor([0, 0, 0, 0, 1, 0], dtype=th.long),\n    \"aegypti\": th.tensor([0, 0, 0, 0, 0, 1], dtype=th.long),\n}\n\ntransform = pre_process(\"\")\n\ndata_augmentation = aug(\"image_net\")\n\nds = SimpleClassificationDataset(\n    annotations_df=annotations_df,\n    img_dir=img_dir,\n    class_dict=class_dict,\n    transform=transform,\n    data_augment=data_augmentation,\n)\nfor i in range(10):\n    res = ds[i]\n    img = res[\"img\"]\n\n    img_bbox = th.tensor(\n        255 * (img - img.min()) / (img.max() - img.min()), dtype=th.uint8\n    )\n    show(img_bbox)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:43:03.456657Z","iopub.execute_input":"2023-09-09T17:43:03.457029Z","iopub.status.idle":"2023-09-09T17:43:08.602497Z","shell.execute_reply.started":"2023-09-09T17:43:03.456997Z","shell.execute_reply":"2023-09-09T17:43:08.601550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = ExperimentMosquitoClassifier(img_dir, annotations_csv)\n\nparams = []\nfor data_aug in [\"image_net\", \"happy_whale\", \"hca\"]:\n    for fb in [False]:\n        for warm_up_steps in [1000, 1500]:\n            for head_version in [2]:\n                for model in [\n                    [\"ViT-B-16\", \"datacomp_l_s1b_b8k\", 64],\n                    [\"ViT-L-14\", \"datacomp_xl_s13b_b90k\", 64],\n                ]:\n                    param = model + [head_version, data_aug, fb, warm_up_steps]\n                    params.append(param)\n\nfor data_aug in [\"image_net\", \"happy_whale\", \"hca\"]:\n    for fb in [True]:\n        for warm_up_steps in [100, 0]:\n            for head_version in [2]:\n                for model in [\n                    [\"ViT-B-16\", \"datacomp_l_s1b_b8k\", 64],\n                    [\"ViT-L-14\", \"datacomp_xl_s13b_b90k\", 64],\n                ]:\n                    param = model + [head_version, data_aug, fb, warm_up_steps]\n                    params.append(param)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:43:08.604247Z","iopub.execute_input":"2023-09-09T17:43:08.606894Z","iopub.status.idle":"2023-09-09T17:43:08.616264Z","shell.execute_reply.started":"2023-09-09T17:43:08.606857Z","shell.execute_reply":"2023-09-09T17:43:08.614745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = [['ViT-L-14', 'datacomp_xl_s13b_b90k', 64, 2, 'hca', True, 0]]","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:46:05.095312Z","iopub.execute_input":"2023-09-09T17:46:05.095695Z","iopub.status.idle":"2023-09-09T17:46:05.101058Z","shell.execute_reply.started":"2023-09-09T17:46:05.095662Z","shell.execute_reply":"2023-09-09T17:46:05.099944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total experiments {len(params)}\")\nfor param in params:\n    print(\"Params:\", param)\n    exp.run(*param)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T17:46:05.344733Z","iopub.execute_input":"2023-09-09T17:46:05.345083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bounding Boxes","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nfrom pathlib import Path\nimport cv2\nfrom timeit import default_timer as timer\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:59:37.502496Z","iopub.execute_input":"2023-09-13T17:59:37.502892Z","iopub.status.idle":"2023-09-13T17:59:37.510389Z","shell.execute_reply.started":"2023-09-13T17:59:37.502859Z","shell.execute_reply":"2023-09-13T17:59:37.509226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/')\nowl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\", cache_dir='models/owl/').cuda()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:01:58.801439Z","iopub.execute_input":"2023-09-13T17:01:58.802670Z","iopub.status.idle":"2023-09-13T17:02:29.478866Z","shell.execute_reply.started":"2023-09-13T17:01:58.802618Z","shell.execute_reply":"2023-09-13T17:02:29.477750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0/final/\"\nannotations_csv = \"/kaggle/input/mosquito-data-round-2/phase2_train_v0.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:54:07.897955Z","iopub.execute_input":"2023-09-13T17:54:07.898374Z","iopub.status.idle":"2023-09-13T17:54:07.904183Z","shell.execute_reply.started":"2023-09-13T17:54:07.898338Z","shell.execute_reply":"2023-09-13T17:54:07.902701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_images = os.path.join(img_dir)\n\nall_images = os.listdir(root_images)\nprint(f\"Total images: {len(all_images)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:04:48.792264Z","iopub.execute_input":"2023-09-13T17:04:48.793299Z","iopub.status.idle":"2023-09-13T17:04:48.992179Z","shell.execute_reply.started":"2023-09-13T17:04:48.793250Z","shell.execute_reply":"2023-09-13T17:04:48.991124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = []\nfor original_image in tqdm(all_images):\n    original_image_file = os.path.join(root_images, original_image)\n    image = Image.open(original_image_file)\n    with torch.no_grad():\n        texts = [[\"a photo of a mosquito\"]]\n        inputs = owl_processor(text=texts, images=image, return_tensors=\"pt\").to('cuda')\n        outputs = owl_model(**inputs)\n        # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n        target_sizes = torch.Tensor([image.size[::-1]]).to('cuda')\n\n        # Convert outputs (bounding boxes and class logits) to COCO API\n        results = owl_processor.post_process_object_detection(outputs=outputs, \n                                                            target_sizes=target_sizes, \n                                                            threshold=0.01)\n        \n        i = 0  # Retrieve predictions for the first image for the corresponding text queries\n        text = texts[i]\n        boxes, scores, labels = results[i][\"boxes\"].cpu().numpy(), results[i][\"scores\"].cpu().numpy(), results[i][\"labels\"].cpu().numpy()\n        best_score_index = np.argmax(scores)\n        boxes, scores, labels = boxes[best_score_index], scores[best_score_index], labels[best_score_index]\n        row = [original_image, boxes[0], boxes[1], boxes[2], boxes[3]]\n        rows.append(row)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:04:52.861548Z","iopub.execute_input":"2023-09-13T17:04:52.861957Z","iopub.status.idle":"2023-09-13T17:52:51.144720Z","shell.execute_reply.started":"2023-09-13T17:04:52.861924Z","shell.execute_reply":"2023-09-13T17:52:51.143441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(rows, columns=[\"img_fName\", \"bbx_xtl\", \"bbx_ytl\", \"bbx_xbr\", \"bbx_ybr\"])\ndf.to_csv('/kaggle/working/owl_vit_image_bboxes.csv', index=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:52:51.147225Z","iopub.execute_input":"2023-09-13T17:52:51.148143Z","iopub.status.idle":"2023-09-13T17:52:51.286473Z","shell.execute_reply.started":"2023-09-13T17:52:51.148099Z","shell.execute_reply":"2023-09-13T17:52:51.285426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df = pd.read_csv(annotations_csv)\nannotations_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:54:13.218662Z","iopub.execute_input":"2023-09-13T17:54:13.219079Z","iopub.status.idle":"2023-09-13T17:54:13.268409Z","shell.execute_reply.started":"2023-09-13T17:54:13.219045Z","shell.execute_reply":"2023-09-13T17:54:13.266252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged = annotations_df.merge(df, on='img_fName', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:56:09.179848Z","iopub.execute_input":"2023-09-13T17:56:09.180310Z","iopub.status.idle":"2023-09-13T17:56:09.214094Z","shell.execute_reply.started":"2023-09-13T17:56:09.180272Z","shell.execute_reply":"2023-09-13T17:56:09.212985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['bbox_true'] = df_merged.apply(lambda x: [x.bbx_xtl_x, x.bbx_ytl_x, x.bbx_xbr_x, x.bbx_ybr_x], axis=1)\ndf_merged['bbox_owl'] = df_merged.apply(lambda x: [x.bbx_xtl_y, x.bbx_ytl_y, x.bbx_xbr_y, x.bbx_ybr_y], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:56:34.951829Z","iopub.execute_input":"2023-09-13T17:56:34.952238Z","iopub.status.idle":"2023-09-13T17:56:35.983719Z","shell.execute_reply.started":"2023-09-13T17:56:34.952207Z","shell.execute_reply":"2023-09-13T17:56:35.982585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:56:57.267091Z","iopub.execute_input":"2023-09-13T17:56:57.267546Z","iopub.status.idle":"2023-09-13T17:56:57.277214Z","shell.execute_reply.started":"2023-09-13T17:56:57.267511Z","shell.execute_reply":"2023-09-13T17:56:57.276147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['IoU'] = df_merged.apply(lambda x: bb_intersection_over_union(x.bbox_true, x.bbox_owl), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:57:07.570641Z","iopub.execute_input":"2023-09-13T17:57:07.571037Z","iopub.status.idle":"2023-09-13T17:57:07.997482Z","shell.execute_reply.started":"2023-09-13T17:57:07.571001Z","shell.execute_reply":"2023-09-13T17:57:07.996454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['IoU'].median()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:04:39.979742Z","iopub.execute_input":"2023-09-13T18:04:39.980432Z","iopub.status.idle":"2023-09-13T18:04:39.998555Z","shell.execute_reply.started":"2023-09-13T18:04:39.980363Z","shell.execute_reply":"2023-09-13T18:04:39.996978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged[df_merged['IoU']<0.1]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T17:57:51.789215Z","iopub.execute_input":"2023-09-13T17:57:51.789626Z","iopub.status.idle":"2023-09-13T17:57:51.832226Z","shell.execute_reply.started":"2023-09-13T17:57:51.789592Z","shell.execute_reply":"2023-09-13T17:57:51.830806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 10224\n\nim = Image.open(img_dir+df_merged['img_fName'].iloc[idx])\n\n# Create figure and axes\nfig, ax = plt.subplots()\n\n# Display the image\nax.imshow(im)\n\n# Create a Rectangle patch\nrect_baseline = patches.Rectangle(\n    df_merged['bbox_true'].iloc[idx][:2], \n    df_merged['bbox_true'].iloc[idx][2]-df_merged['bbox_true'].iloc[idx][0],\n    df_merged['bbox_true'].iloc[idx][3]-df_merged['bbox_true'].iloc[idx][1], linewidth=1, edgecolor='b', facecolor='none', label='ground truth')\n# Add the patch to the Axes\nax.add_patch(rect_baseline)\n\n# Create a Rectangle patch\nrect_owl = patches.Rectangle(\n    df_merged['bbox_owl'].iloc[idx][:2], \n    df_merged['bbox_owl'].iloc[idx][2]-df_merged['bbox_owl'].iloc[idx][0],\n    df_merged['bbox_owl'].iloc[idx][3]-df_merged['bbox_owl'].iloc[idx][1], linewidth=1, edgecolor='r', facecolor='none', label='Owl-ViT')\n# Add the patch to the Axes\nax.add_patch(rect_owl)\nplt.legend()\nplt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:07:55.918725Z","iopub.execute_input":"2023-09-13T18:07:55.919133Z","iopub.status.idle":"2023-09-13T18:07:56.236393Z","shell.execute_reply.started":"2023-09-13T18:07:55.919098Z","shell.execute_reply":"2023-09-13T18:07:56.235372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}